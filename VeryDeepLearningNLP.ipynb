{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 (NLP): Very Deep Learning\n",
    "\n",
    "**Natural language processing (NLP)** is the ability of a computer program to understand human language as it is spoken. It involves a pipeline of steps and by the end of the exercise, we would be able to classify the sentiment of a given review as POSITIVE or NEGATIVE.\n",
    "\n",
    "\n",
    "Before starting, it is important to understand the need for RNNs and the lecture from Stanford is a must to see before starting the exercise:\n",
    "\n",
    "https://www.youtube.com/watch?v=iX5V1WpxxkY\n",
    "\n",
    "When done, let's begin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this exercise, we will import libraries when needed so that we understand the need for it. \n",
    "# However, this is a bad practice and don't get used to it.\n",
    "import numpy as np\n",
    "\n",
    "# read data from reviews and labels file.\n",
    "with open('data/reviews.txt', 'r') as f:\n",
    "    reviews_ = f.readlines()\n",
    "with open('data/labels.txt', 'r') as f:\n",
    "    \n",
    "    labels = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "\t: bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life...\n",
      "negative\n",
      "\t: story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terr...\n",
      "positive\n",
      "\t: homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan...\n",
      "negative\n",
      "\t: airport    starts as a brand new luxury    plane is loaded up with valuable paintings  such belongin...\n",
      "positive\n",
      "\t: brilliant over  acting by lesley ann warren . best dramatic hobo lady i have ever seen  and love sce...\n"
     ]
    }
   ],
   "source": [
    "# One of the most important task is to visualize data before starting with any ML task. \n",
    "for i in range(5):\n",
    "    print(labels[i] + \"\\t: \" + reviews_[i][:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We can see there are a lot of punctuation marks like fullstop(.), comma(,), new line (\\n) and so on and we need to remove it. \n",
    "\n",
    "Here is a list of all the punctuation marks that needs to be removed \n",
    "```\n",
    "(!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Remove all the punctuation marks from the reviews.\n",
    "Many ways of doing it: Regex, Spacy, import punctuation from string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make everything lower case to make the whole dataset even. \n",
    "reviews = ''.join(reviews_).lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('labels-', 25000, 'reviews ', 25000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers   the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students  when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled          at           high  a classic line inspector i  m here to sack one of your teachers  student welcome to bromwell high  i expect that many adults of my age think that bromwell high is far fetched  what a pity that it isn  t   '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# complete the function below to remove punctuations and save it in no_punct_text\n",
    "import re\n",
    "\n",
    "def text_without_punct(reviews):\n",
    "    spl_char = '[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+'\n",
    "    #return re.sub('[^A-Za-z0-9]+','',reviews)\n",
    "    return re.sub(spl_char,'',reviews).strip()\n",
    "\n",
    "\n",
    "no_punct_text = text_without_punct(reviews)\n",
    "reviews_split = no_punct_text.split('\\n')\n",
    "print('labels-',len(labels),'reviews ',len(reviews_split))\n",
    "reviews_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the formatted no_punct_text into words\n",
    "def split_in_words(no_punct_text):\n",
    "    return no_punct_text.split()\n",
    "\n",
    "words = split_in_words(no_punct_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# once you are done print the ten words that should yield the following output\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6020196"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the total length of the words\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74072"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of unique words\n",
    "len(set(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next step is to create a vocabulary. This way every word is mapped to an integer number.\n",
    "```\n",
    "Example: 1: hello, 2: I, 3: am, 4: Robo and so on...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a vocab out of it\n",
    "\n",
    "# feel free to use this import \n",
    "from collections import Counter\n",
    "\n",
    "## Let's keep a count of all the words and let's see how many words are there. \n",
    "def word_count(words):\n",
    "    return Counter(words)\n",
    "\n",
    "counts=word_count(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1658\n",
      "9308\n"
     ]
    }
   ],
   "source": [
    "# If you did everything correct, this is what you should get as output. \n",
    "print (counts['wonderful'])\n",
    "\n",
    "print (counts['bad'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Word to Integer and Integer to word\n",
    "The task is to map every word to an integer value and then vice-versa. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tsukino'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a vocabulary for the words\n",
    "def vocabulary(counts):\n",
    "    vocab = []\n",
    "    for c in counts:\n",
    "        vocab.append(c)\n",
    "    return vocab\n",
    "\n",
    "vocab = vocabulary(counts)\n",
    "print(len(vocab))\n",
    "vocab[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map each vocab word to an integer. Also, start the indexing with 1 as we will use \n",
    "# '0' for padding and we dont want to mix the two.\n",
    "def vocabulary_to_integer(vocab):\n",
    "    vocab_to_int = {}\n",
    "    for i in range(len(vocab)):\n",
    "        vocab_to_int[vocab[i]] = i\n",
    "    return vocab_to_int\n",
    "\n",
    "vocab_to_int = vocabulary_to_integer(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify if the length is same and if 'and' is mapped to the correct integer value.\n",
    "print(len(vocab_to_int))\n",
    "vocab_to_int['tsukino']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what positve words in positive reviews we have and what we have in negative reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_counts = Counter()\n",
    "negative_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over each sentence\n",
    "for i in range(len(reviews_)):\n",
    "    # if the sentence has positive review, all the words contained in the sentence contribute +1 to positive_counts\n",
    "    if(labels[i] == 'positive\\n'):\n",
    "        for word in reviews_[i].split(\" \"):\n",
    "            positive_counts[word] += 1\n",
    "    # if the sentence has negative review, all the words contained in the sentence contribute +1 to negative_counts\n",
    "    else:\n",
    "        for word in reviews_[i].split(\" \"):\n",
    "            negative_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 537968),\n",
       " ('the', 173324),\n",
       " ('.', 159654),\n",
       " ('and', 89722),\n",
       " ('a', 83688),\n",
       " ('of', 76855),\n",
       " ('to', 66746),\n",
       " ('is', 57245),\n",
       " ('in', 50215),\n",
       " ('br', 49235),\n",
       " ('it', 48025),\n",
       " ('i', 40743),\n",
       " ('that', 35630),\n",
       " ('this', 35080),\n",
       " ('s', 33815),\n",
       " ('as', 26308),\n",
       " ('with', 23247),\n",
       " ('for', 22416),\n",
       " ('was', 21917),\n",
       " ('film', 20937),\n",
       " ('but', 20822),\n",
       " ('movie', 19074),\n",
       " ('his', 17227),\n",
       " ('on', 17008),\n",
       " ('you', 16681),\n",
       " ('he', 16282),\n",
       " ('are', 14807),\n",
       " ('not', 14272),\n",
       " ('t', 13720),\n",
       " ('one', 13655),\n",
       " ('have', 12587),\n",
       " ('\\n', 12500),\n",
       " ('be', 12416),\n",
       " ('by', 11997),\n",
       " ('all', 11942),\n",
       " ('who', 11464),\n",
       " ('an', 11294),\n",
       " ('at', 11234),\n",
       " ('from', 10767),\n",
       " ('her', 10474),\n",
       " ('they', 9895),\n",
       " ('has', 9186),\n",
       " ('so', 9154),\n",
       " ('like', 9038),\n",
       " ('about', 8313),\n",
       " ('very', 8305),\n",
       " ('out', 8134),\n",
       " ('there', 8057),\n",
       " ('she', 7779),\n",
       " ('what', 7737),\n",
       " ('or', 7732),\n",
       " ('good', 7720),\n",
       " ('more', 7521),\n",
       " ('when', 7456),\n",
       " ('some', 7441),\n",
       " ('if', 7285),\n",
       " ('just', 7152),\n",
       " ('can', 7001),\n",
       " ('story', 6780),\n",
       " ('time', 6515),\n",
       " ('my', 6488),\n",
       " ('great', 6419),\n",
       " ('well', 6405),\n",
       " ('up', 6321),\n",
       " ('which', 6267),\n",
       " ('their', 6107),\n",
       " ('see', 6026),\n",
       " ('also', 5550),\n",
       " ('we', 5531),\n",
       " ('really', 5476),\n",
       " ('would', 5400),\n",
       " ('will', 5218),\n",
       " ('me', 5167),\n",
       " ('had', 5148),\n",
       " ('only', 5137),\n",
       " ('him', 5018),\n",
       " ('even', 4964),\n",
       " ('most', 4864),\n",
       " ('other', 4858),\n",
       " ('were', 4782),\n",
       " ('first', 4755),\n",
       " ('than', 4736),\n",
       " ('much', 4685),\n",
       " ('its', 4622),\n",
       " ('no', 4574),\n",
       " ('into', 4544),\n",
       " ('people', 4479),\n",
       " ('best', 4319),\n",
       " ('love', 4301),\n",
       " ('get', 4272),\n",
       " ('how', 4213),\n",
       " ('life', 4199),\n",
       " ('been', 4189),\n",
       " ('because', 4079),\n",
       " ('way', 4036),\n",
       " ('do', 3941),\n",
       " ('made', 3823),\n",
       " ('films', 3813),\n",
       " ('them', 3805),\n",
       " ('after', 3800),\n",
       " ('many', 3766),\n",
       " ('two', 3733),\n",
       " ('too', 3659),\n",
       " ('think', 3655),\n",
       " ('movies', 3586),\n",
       " ('characters', 3560),\n",
       " ('character', 3514),\n",
       " ('don', 3468),\n",
       " ('man', 3460),\n",
       " ('show', 3432),\n",
       " ('watch', 3424),\n",
       " ('seen', 3414),\n",
       " ('then', 3358),\n",
       " ('little', 3341),\n",
       " ('still', 3340),\n",
       " ('make', 3303),\n",
       " ('could', 3237),\n",
       " ('never', 3226),\n",
       " ('being', 3217),\n",
       " ('where', 3173),\n",
       " ('does', 3069),\n",
       " ('over', 3017),\n",
       " ('any', 3002),\n",
       " ('while', 2899),\n",
       " ('know', 2833),\n",
       " ('did', 2790),\n",
       " ('years', 2758),\n",
       " ('here', 2740),\n",
       " ('ever', 2734),\n",
       " ('end', 2696),\n",
       " ('these', 2694),\n",
       " ('such', 2590),\n",
       " ('real', 2568),\n",
       " ('scene', 2567),\n",
       " ('back', 2547),\n",
       " ('those', 2485),\n",
       " ('though', 2475),\n",
       " ('off', 2463),\n",
       " ('new', 2458),\n",
       " ('your', 2453),\n",
       " ('go', 2440),\n",
       " ('acting', 2437),\n",
       " ('plot', 2432),\n",
       " ('world', 2429),\n",
       " ('scenes', 2427),\n",
       " ('say', 2414),\n",
       " ('through', 2409),\n",
       " ('makes', 2390),\n",
       " ('better', 2381),\n",
       " ('now', 2368),\n",
       " ('work', 2346),\n",
       " ('young', 2343),\n",
       " ('old', 2311),\n",
       " ('ve', 2307),\n",
       " ('find', 2272),\n",
       " ('both', 2248),\n",
       " ('before', 2177),\n",
       " ('us', 2162),\n",
       " ('again', 2158),\n",
       " ('series', 2153),\n",
       " ('quite', 2143),\n",
       " ('something', 2135),\n",
       " ('cast', 2133),\n",
       " ('should', 2121),\n",
       " ('part', 2098),\n",
       " ('always', 2088),\n",
       " ('lot', 2087),\n",
       " ('another', 2075),\n",
       " ('actors', 2047),\n",
       " ('director', 2040),\n",
       " ('family', 2032),\n",
       " ('own', 2016),\n",
       " ('between', 2016),\n",
       " ('m', 1998),\n",
       " ('may', 1997),\n",
       " ('same', 1972),\n",
       " ('role', 1967),\n",
       " ('watching', 1966),\n",
       " ('every', 1954),\n",
       " ('funny', 1953),\n",
       " ('doesn', 1935),\n",
       " ('performance', 1928),\n",
       " ('few', 1918),\n",
       " ('bad', 1907),\n",
       " ('look', 1900),\n",
       " ('re', 1884),\n",
       " ('why', 1855),\n",
       " ('things', 1849),\n",
       " ('times', 1832),\n",
       " ('big', 1815),\n",
       " ('however', 1795),\n",
       " ('actually', 1790),\n",
       " ('action', 1789),\n",
       " ('going', 1783),\n",
       " ('bit', 1757),\n",
       " ('comedy', 1742),\n",
       " ('down', 1740),\n",
       " ('music', 1738),\n",
       " ('must', 1728),\n",
       " ('take', 1709),\n",
       " ('saw', 1692),\n",
       " ('long', 1690),\n",
       " ('right', 1688),\n",
       " ('fun', 1686),\n",
       " ('fact', 1684),\n",
       " ('excellent', 1683),\n",
       " ('around', 1674),\n",
       " ('didn', 1672),\n",
       " ('without', 1671),\n",
       " ('thing', 1662),\n",
       " ('thought', 1639),\n",
       " ('got', 1635),\n",
       " ('each', 1630),\n",
       " ('day', 1614),\n",
       " ('feel', 1597),\n",
       " ('seems', 1596),\n",
       " ('come', 1594),\n",
       " ('done', 1586),\n",
       " ('beautiful', 1580),\n",
       " ('especially', 1572),\n",
       " ('played', 1571),\n",
       " ('almost', 1566),\n",
       " ('want', 1562),\n",
       " ('yet', 1556),\n",
       " ('give', 1553),\n",
       " ('pretty', 1549),\n",
       " ('last', 1543),\n",
       " ('since', 1519),\n",
       " ('different', 1504),\n",
       " ('although', 1501),\n",
       " ('gets', 1490),\n",
       " ('true', 1487),\n",
       " ('interesting', 1481),\n",
       " ('job', 1470),\n",
       " ('enough', 1455),\n",
       " ('our', 1454),\n",
       " ('shows', 1447),\n",
       " ('horror', 1441),\n",
       " ('woman', 1439),\n",
       " ('tv', 1400),\n",
       " ('probably', 1398),\n",
       " ('father', 1395),\n",
       " ('original', 1393),\n",
       " ('girl', 1390),\n",
       " ('point', 1379),\n",
       " ('plays', 1378),\n",
       " ('wonderful', 1372),\n",
       " ('course', 1358),\n",
       " ('far', 1358),\n",
       " ('john', 1350),\n",
       " ('rather', 1340),\n",
       " ('isn', 1328),\n",
       " ('ll', 1326),\n",
       " ('dvd', 1324),\n",
       " ('later', 1324),\n",
       " ('whole', 1310),\n",
       " ('war', 1310),\n",
       " ('d', 1307),\n",
       " ('away', 1306),\n",
       " ('found', 1306),\n",
       " ('screen', 1305),\n",
       " ('nothing', 1300),\n",
       " ('year', 1297),\n",
       " ('once', 1296),\n",
       " ('hard', 1294),\n",
       " ('together', 1280),\n",
       " ('am', 1277),\n",
       " ('set', 1277),\n",
       " ('having', 1266),\n",
       " ('making', 1265),\n",
       " ('place', 1263),\n",
       " ('comes', 1260),\n",
       " ('might', 1260),\n",
       " ('sure', 1253),\n",
       " ('american', 1248),\n",
       " ('play', 1245),\n",
       " ('kind', 1244),\n",
       " ('perfect', 1242),\n",
       " ('takes', 1242),\n",
       " ('performances', 1237),\n",
       " ('himself', 1230),\n",
       " ('worth', 1221),\n",
       " ('everyone', 1221),\n",
       " ('anyone', 1214),\n",
       " ('actor', 1203),\n",
       " ('three', 1201),\n",
       " ('wife', 1196),\n",
       " ('classic', 1192),\n",
       " ('goes', 1186),\n",
       " ('ending', 1178),\n",
       " ('version', 1168),\n",
       " ('star', 1149),\n",
       " ('enjoy', 1146),\n",
       " ('book', 1142),\n",
       " ('nice', 1132),\n",
       " ('everything', 1128),\n",
       " ('during', 1124),\n",
       " ('put', 1118),\n",
       " ('seeing', 1111),\n",
       " ('least', 1102),\n",
       " ('house', 1100),\n",
       " ('high', 1095),\n",
       " ('watched', 1094),\n",
       " ('men', 1087),\n",
       " ('loved', 1087),\n",
       " ('night', 1082),\n",
       " ('anything', 1075),\n",
       " ('guy', 1071),\n",
       " ('believe', 1071),\n",
       " ('top', 1063),\n",
       " ('amazing', 1058),\n",
       " ('hollywood', 1056),\n",
       " ('looking', 1053),\n",
       " ('main', 1044),\n",
       " ('definitely', 1043),\n",
       " ('gives', 1031),\n",
       " ('home', 1029),\n",
       " ('seem', 1028),\n",
       " ('episode', 1023),\n",
       " ('sense', 1020),\n",
       " ('audience', 1020),\n",
       " ('truly', 1017),\n",
       " ('special', 1011),\n",
       " ('short', 1009),\n",
       " ('second', 1009),\n",
       " ('fan', 1009),\n",
       " ('mind', 1005),\n",
       " ('human', 1001),\n",
       " ('recommend', 999),\n",
       " ('full', 996),\n",
       " ('black', 995),\n",
       " ('help', 991),\n",
       " ('along', 989),\n",
       " ('trying', 987),\n",
       " ('small', 986),\n",
       " ('death', 985),\n",
       " ('friends', 981),\n",
       " ('remember', 974),\n",
       " ('often', 970),\n",
       " ('said', 966),\n",
       " ('favorite', 962),\n",
       " ('heart', 959),\n",
       " ('early', 957),\n",
       " ('left', 956),\n",
       " ('until', 955),\n",
       " ('let', 954),\n",
       " ('script', 954),\n",
       " ('maybe', 937),\n",
       " ('today', 936),\n",
       " ('live', 934),\n",
       " ('less', 934),\n",
       " ('moments', 933),\n",
       " ('others', 929),\n",
       " ('brilliant', 926),\n",
       " ('shot', 925),\n",
       " ('liked', 923),\n",
       " ('become', 916),\n",
       " ('won', 915),\n",
       " ('used', 910),\n",
       " ('style', 907),\n",
       " ('mother', 895),\n",
       " ('lives', 894),\n",
       " ('came', 893),\n",
       " ('stars', 890),\n",
       " ('cinema', 889),\n",
       " ('looks', 885),\n",
       " ('perhaps', 884),\n",
       " ('read', 882),\n",
       " ('enjoyed', 879),\n",
       " ('boy', 875),\n",
       " ('drama', 873),\n",
       " ('highly', 871),\n",
       " ('given', 870),\n",
       " ('playing', 867),\n",
       " ('use', 864),\n",
       " ('next', 859),\n",
       " ('women', 858),\n",
       " ('fine', 857),\n",
       " ('effects', 856),\n",
       " ('kids', 854),\n",
       " ('entertaining', 853),\n",
       " ('need', 852),\n",
       " ('line', 850),\n",
       " ('works', 848),\n",
       " ('someone', 847),\n",
       " ('mr', 836),\n",
       " ('simply', 835),\n",
       " ('picture', 833),\n",
       " ('children', 833),\n",
       " ('friend', 831),\n",
       " ('keep', 831),\n",
       " ('face', 831),\n",
       " ('dark', 830),\n",
       " ('certainly', 828),\n",
       " ('overall', 828),\n",
       " ('minutes', 827),\n",
       " ('wasn', 824),\n",
       " ('history', 822),\n",
       " ('finally', 820),\n",
       " ('couple', 816),\n",
       " ('against', 815),\n",
       " ('son', 809),\n",
       " ('understand', 808),\n",
       " ('lost', 807),\n",
       " ('michael', 805),\n",
       " ('else', 801),\n",
       " ('throughout', 798),\n",
       " ('fans', 797),\n",
       " ('city', 792),\n",
       " ('reason', 789),\n",
       " ('production', 787),\n",
       " ('written', 787),\n",
       " ('several', 784),\n",
       " ('school', 783),\n",
       " ('rest', 781),\n",
       " ('based', 781),\n",
       " ('try', 780),\n",
       " ('dead', 776),\n",
       " ('hope', 775),\n",
       " ('strong', 768),\n",
       " ('white', 765),\n",
       " ('tell', 759),\n",
       " ('itself', 758),\n",
       " ('half', 753),\n",
       " ('person', 749),\n",
       " ('sometimes', 746),\n",
       " ('past', 744),\n",
       " ('start', 744),\n",
       " ('genre', 743),\n",
       " ('beginning', 739),\n",
       " ('final', 739),\n",
       " ('town', 738),\n",
       " ('art', 734),\n",
       " ('game', 732),\n",
       " ('humor', 732),\n",
       " ('yes', 731),\n",
       " ('idea', 731),\n",
       " ('late', 730),\n",
       " ('despite', 729),\n",
       " ('becomes', 729),\n",
       " ('case', 726),\n",
       " ('able', 726),\n",
       " ('money', 723),\n",
       " ('child', 721),\n",
       " ('completely', 721),\n",
       " ('side', 719),\n",
       " ('camera', 716),\n",
       " ('getting', 714),\n",
       " ('instead', 712),\n",
       " ('soon', 702),\n",
       " ('under', 700),\n",
       " ('viewer', 699),\n",
       " ('age', 697),\n",
       " ('days', 696),\n",
       " ('stories', 696),\n",
       " ('felt', 694),\n",
       " ('simple', 694),\n",
       " ('roles', 693),\n",
       " ('video', 688),\n",
       " ('either', 683),\n",
       " ('name', 683),\n",
       " ('doing', 677),\n",
       " ('turns', 674),\n",
       " ('wants', 671),\n",
       " ('close', 671),\n",
       " ('title', 669),\n",
       " ('wrong', 668),\n",
       " ('went', 666),\n",
       " ('james', 665),\n",
       " ('evil', 659),\n",
       " ('episodes', 657),\n",
       " ('budget', 657),\n",
       " ('relationship', 655),\n",
       " ('fantastic', 653),\n",
       " ('piece', 653),\n",
       " ('david', 651),\n",
       " ('turn', 648),\n",
       " ('murder', 646),\n",
       " ('parts', 645),\n",
       " ('brother', 644),\n",
       " ('absolutely', 643),\n",
       " ('head', 643),\n",
       " ('experience', 642),\n",
       " ('eyes', 641),\n",
       " ('sex', 638),\n",
       " ('direction', 637),\n",
       " ('called', 637),\n",
       " ('directed', 636),\n",
       " ('lines', 634),\n",
       " ('behind', 633),\n",
       " ('sort', 632),\n",
       " ('actress', 631),\n",
       " ('lead', 630),\n",
       " ('oscar', 628),\n",
       " ('example', 627),\n",
       " ('including', 627),\n",
       " ('musical', 625),\n",
       " ('known', 625),\n",
       " ('chance', 621),\n",
       " ('score', 620),\n",
       " ('feeling', 619),\n",
       " ('already', 619),\n",
       " ('hit', 619),\n",
       " ('voice', 615),\n",
       " ('moment', 612),\n",
       " ('living', 612),\n",
       " ('supporting', 610),\n",
       " ('low', 610),\n",
       " ('ago', 609),\n",
       " ('themselves', 608),\n",
       " ('hilarious', 605),\n",
       " ('reality', 605),\n",
       " ('jack', 604),\n",
       " ('told', 603),\n",
       " ('hand', 601),\n",
       " ('dialogue', 600),\n",
       " ('quality', 600),\n",
       " ('moving', 600),\n",
       " ('happy', 599),\n",
       " ('song', 599),\n",
       " ('matter', 598),\n",
       " ('paul', 598),\n",
       " ('light', 594),\n",
       " ('future', 593),\n",
       " ('entire', 592),\n",
       " ('finds', 591),\n",
       " ('gave', 589),\n",
       " ('laugh', 587),\n",
       " ('released', 586),\n",
       " ('expect', 584),\n",
       " ('fight', 581),\n",
       " ('particularly', 580),\n",
       " ('cinematography', 579),\n",
       " ('police', 579),\n",
       " ('sound', 578),\n",
       " ('type', 578),\n",
       " ('whose', 578),\n",
       " ('enjoyable', 573),\n",
       " ('view', 573),\n",
       " ('number', 572),\n",
       " ('husband', 572),\n",
       " ('daughter', 572),\n",
       " ('romantic', 572),\n",
       " ('documentary', 571),\n",
       " ('self', 570),\n",
       " ('robert', 569),\n",
       " ('took', 569),\n",
       " ('modern', 569),\n",
       " ('superb', 569),\n",
       " ('mean', 566),\n",
       " ('shown', 563),\n",
       " ('coming', 561),\n",
       " ('important', 560),\n",
       " ('king', 559),\n",
       " ('leave', 559),\n",
       " ('change', 558),\n",
       " ('somewhat', 555),\n",
       " ('wanted', 555),\n",
       " ('tells', 554),\n",
       " ('run', 552),\n",
       " ('events', 552),\n",
       " ('country', 552),\n",
       " ('career', 552),\n",
       " ('heard', 550),\n",
       " ('season', 550),\n",
       " ('girls', 549),\n",
       " ('greatest', 549),\n",
       " ('etc', 547),\n",
       " ('care', 546),\n",
       " ('starts', 545),\n",
       " ('english', 542),\n",
       " ('killer', 541),\n",
       " ('animation', 540),\n",
       " ('guys', 540),\n",
       " ('totally', 540),\n",
       " ('tale', 540),\n",
       " ('usual', 539),\n",
       " ('opinion', 535),\n",
       " ('miss', 535),\n",
       " ('easy', 531),\n",
       " ('violence', 531),\n",
       " ('songs', 530),\n",
       " ('british', 528),\n",
       " ('says', 526),\n",
       " ('realistic', 525),\n",
       " ('writing', 524),\n",
       " ('act', 522),\n",
       " ('writer', 522),\n",
       " ('comic', 521),\n",
       " ('thriller', 519),\n",
       " ('television', 517),\n",
       " ('power', 516),\n",
       " ('ones', 515),\n",
       " ('kid', 514),\n",
       " ('novel', 513),\n",
       " ('york', 513),\n",
       " ('problem', 512),\n",
       " ('alone', 512),\n",
       " ('attention', 509),\n",
       " ('involved', 508),\n",
       " ('extremely', 507),\n",
       " ('kill', 507),\n",
       " ('seemed', 506),\n",
       " ('hero', 505),\n",
       " ('french', 505),\n",
       " ('rock', 504),\n",
       " ('stuff', 501),\n",
       " ('wish', 499),\n",
       " ('begins', 498),\n",
       " ('taken', 497),\n",
       " ('sad', 497),\n",
       " ('ways', 496),\n",
       " ('richard', 495),\n",
       " ('knows', 494),\n",
       " ('atmosphere', 493),\n",
       " ('similar', 491),\n",
       " ('car', 491),\n",
       " ('taking', 491),\n",
       " ('surprised', 491),\n",
       " ('perfectly', 490),\n",
       " ('george', 490),\n",
       " ('across', 489),\n",
       " ('team', 489),\n",
       " ('eye', 489),\n",
       " ('sequence', 489),\n",
       " ('powerful', 488),\n",
       " ('room', 488),\n",
       " ('among', 488),\n",
       " ('due', 488),\n",
       " ('serious', 488),\n",
       " ('b', 487),\n",
       " ('cannot', 487),\n",
       " ('strange', 487),\n",
       " ('order', 487),\n",
       " ('beauty', 486),\n",
       " ('famous', 485),\n",
       " ('herself', 484),\n",
       " ('happened', 484),\n",
       " ('myself', 484),\n",
       " ('tries', 484),\n",
       " ('class', 483),\n",
       " ('four', 482),\n",
       " ('cool', 481),\n",
       " ('theme', 479),\n",
       " ('anyway', 479),\n",
       " ('release', 479),\n",
       " ('opening', 478),\n",
       " ('entertainment', 477),\n",
       " ('unique', 475),\n",
       " ('ends', 475),\n",
       " ('slow', 475),\n",
       " ('exactly', 475),\n",
       " ('level', 474),\n",
       " ('red', 474),\n",
       " ('o', 474),\n",
       " ('easily', 474),\n",
       " ('interest', 472),\n",
       " ('happen', 471),\n",
       " ('crime', 470),\n",
       " ('viewing', 468),\n",
       " ('memorable', 467),\n",
       " ('sets', 467),\n",
       " ('group', 466),\n",
       " ('stop', 466),\n",
       " ('working', 463),\n",
       " ('sister', 463),\n",
       " ('message', 463),\n",
       " ('problems', 463),\n",
       " ('dance', 463),\n",
       " ('knew', 462),\n",
       " ('nature', 461),\n",
       " ('mystery', 461),\n",
       " ('bring', 460),\n",
       " ('brought', 459),\n",
       " ('thinking', 459),\n",
       " ('believable', 459),\n",
       " ('mostly', 458),\n",
       " ('disney', 457),\n",
       " ('couldn', 457),\n",
       " ('society', 456),\n",
       " ('lady', 455),\n",
       " ('within', 455),\n",
       " ('blood', 454),\n",
       " ('parents', 453),\n",
       " ('viewers', 453),\n",
       " ('upon', 453),\n",
       " ('meets', 452),\n",
       " ('form', 452),\n",
       " ('usually', 452),\n",
       " ('peter', 452),\n",
       " ('tom', 452),\n",
       " ('soundtrack', 452),\n",
       " ('local', 450),\n",
       " ('follow', 448),\n",
       " ('certain', 448),\n",
       " ('whether', 447),\n",
       " ('possible', 446),\n",
       " ('emotional', 445),\n",
       " ('above', 444),\n",
       " ('de', 444),\n",
       " ('killed', 444),\n",
       " ('middle', 443),\n",
       " ('god', 443),\n",
       " ('needs', 442),\n",
       " ('happens', 442),\n",
       " ('flick', 442),\n",
       " ('masterpiece', 441),\n",
       " ('period', 440),\n",
       " ('major', 440),\n",
       " ('named', 439),\n",
       " ('haven', 439),\n",
       " ('th', 438),\n",
       " ('particular', 438),\n",
       " ('feature', 437),\n",
       " ('earth', 437),\n",
       " ('stand', 436),\n",
       " ('typical', 435),\n",
       " ('words', 435),\n",
       " ('obviously', 433),\n",
       " ('elements', 433),\n",
       " ('romance', 431),\n",
       " ('jane', 430),\n",
       " ('showing', 427),\n",
       " ('yourself', 427),\n",
       " ('fantasy', 426),\n",
       " ('brings', 426),\n",
       " ('america', 423),\n",
       " ('guess', 423),\n",
       " ('huge', 422),\n",
       " ('unfortunately', 422),\n",
       " ('indeed', 421),\n",
       " ('running', 421),\n",
       " ('talent', 420),\n",
       " ('stage', 419),\n",
       " ('started', 418),\n",
       " ('sweet', 417),\n",
       " ('leads', 417),\n",
       " ('japanese', 417),\n",
       " ('poor', 416),\n",
       " ('deal', 416),\n",
       " ('personal', 413),\n",
       " ('incredible', 413),\n",
       " ('fast', 412),\n",
       " ('deep', 410),\n",
       " ('became', 410),\n",
       " ('hours', 409),\n",
       " ('giving', 408),\n",
       " ('dream', 408),\n",
       " ('nearly', 408),\n",
       " ('turned', 407),\n",
       " ('clearly', 407),\n",
       " ('near', 406),\n",
       " ('obvious', 406),\n",
       " ('cut', 405),\n",
       " ('surprise', 405),\n",
       " ('era', 404),\n",
       " ('body', 404),\n",
       " ('hour', 403),\n",
       " ('female', 403),\n",
       " ('five', 403),\n",
       " ('note', 399),\n",
       " ('learn', 398),\n",
       " ('truth', 398),\n",
       " ('tony', 397),\n",
       " ('feels', 397),\n",
       " ('except', 397),\n",
       " ('match', 397),\n",
       " ('complete', 394),\n",
       " ('clear', 394),\n",
       " ('filmed', 394),\n",
       " ('lots', 393),\n",
       " ('keeps', 393),\n",
       " ('older', 393),\n",
       " ('street', 393),\n",
       " ('eventually', 393),\n",
       " ('buy', 392),\n",
       " ('stewart', 391),\n",
       " ('william', 391),\n",
       " ('meet', 390),\n",
       " ('joe', 390),\n",
       " ('fall', 390),\n",
       " ('talking', 389),\n",
       " ('shots', 389),\n",
       " ('difficult', 389),\n",
       " ('rating', 389),\n",
       " ('unlike', 389),\n",
       " ('dramatic', 388),\n",
       " ('means', 388),\n",
       " ('situation', 386),\n",
       " ('wonder', 386),\n",
       " ('appears', 386),\n",
       " ('subject', 386),\n",
       " ('present', 386),\n",
       " ('comments', 385),\n",
       " ('lee', 383),\n",
       " ('general', 383),\n",
       " ('sequences', 383),\n",
       " ('points', 382),\n",
       " ('earlier', 382),\n",
       " ('gone', 379),\n",
       " ('check', 379),\n",
       " ('suspense', 378),\n",
       " ('ten', 378),\n",
       " ('recommended', 378),\n",
       " ('third', 377),\n",
       " ('business', 377),\n",
       " ('talk', 375),\n",
       " ('leaves', 375),\n",
       " ('beyond', 375),\n",
       " ('portrayal', 374),\n",
       " ('beautifully', 373),\n",
       " ('bill', 372),\n",
       " ('single', 372),\n",
       " ('plenty', 371),\n",
       " ('word', 371),\n",
       " ('falls', 370),\n",
       " ('whom', 370),\n",
       " ('battle', 369),\n",
       " ('non', 369),\n",
       " ('figure', 369),\n",
       " ('scary', 369),\n",
       " ('using', 368),\n",
       " ('return', 368),\n",
       " ('add', 367),\n",
       " ('doubt', 367),\n",
       " ('success', 366),\n",
       " ('hear', 366),\n",
       " ('solid', 366),\n",
       " ('jokes', 365),\n",
       " ('touching', 365),\n",
       " ('oh', 365),\n",
       " ('political', 365),\n",
       " ('awesome', 364),\n",
       " ('hell', 364),\n",
       " ('boys', 364),\n",
       " ('sexual', 362),\n",
       " ('recently', 362),\n",
       " ('dog', 362),\n",
       " ('straight', 361),\n",
       " ('features', 361),\n",
       " ('wouldn', 361),\n",
       " ('please', 361),\n",
       " ('setting', 360),\n",
       " ('forget', 360),\n",
       " ('lack', 360),\n",
       " ('married', 359),\n",
       " ('mark', 359),\n",
       " ('social', 357),\n",
       " ('adventure', 356),\n",
       " ('interested', 356),\n",
       " ('brothers', 355),\n",
       " ('sees', 355),\n",
       " ('actual', 355),\n",
       " ('terrific', 355),\n",
       " ('call', 354),\n",
       " ('move', 354),\n",
       " ('dr', 353),\n",
       " ('theater', 353),\n",
       " ('various', 353),\n",
       " ('animated', 352),\n",
       " ('western', 351),\n",
       " ('baby', 350),\n",
       " ('space', 350),\n",
       " ('leading', 348),\n",
       " ('disappointed', 348),\n",
       " ('portrayed', 346),\n",
       " ('aren', 346),\n",
       " ('smith', 345),\n",
       " ('screenplay', 345),\n",
       " ('towards', 344),\n",
       " ('hate', 344),\n",
       " ('noir', 343),\n",
       " ('decent', 342),\n",
       " ('kelly', 342),\n",
       " ('outstanding', 342),\n",
       " ('journey', 341),\n",
       " ('directors', 341),\n",
       " ('effective', 340),\n",
       " ('none', 340),\n",
       " ('looked', 340),\n",
       " ('sci', 339),\n",
       " ('cold', 339),\n",
       " ('fi', 339),\n",
       " ('caught', 339),\n",
       " ('mary', 339),\n",
       " ('storyline', 339),\n",
       " ('rich', 338),\n",
       " ('charming', 338),\n",
       " ('popular', 337),\n",
       " ('rare', 337),\n",
       " ('harry', 337),\n",
       " ('manages', 337),\n",
       " ('spirit', 336),\n",
       " ('appreciate', 335),\n",
       " ('open', 335),\n",
       " ('acted', 334),\n",
       " ('moves', 334),\n",
       " ('basically', 334),\n",
       " ('deserves', 333),\n",
       " ('pace', 333),\n",
       " ('century', 333),\n",
       " ('subtle', 333),\n",
       " ('boring', 333),\n",
       " ('inside', 333),\n",
       " ('mention', 333),\n",
       " ('familiar', 332),\n",
       " ('background', 332),\n",
       " ('ben', 331),\n",
       " ('supposed', 330),\n",
       " ('creepy', 330),\n",
       " ('secret', 329),\n",
       " ('jim', 328),\n",
       " ('die', 328),\n",
       " ('effect', 327),\n",
       " ('question', 327),\n",
       " ('natural', 327),\n",
       " ('impressive', 326),\n",
       " ('language', 326),\n",
       " ('rate', 326),\n",
       " ('saying', 325),\n",
       " ('intelligent', 325),\n",
       " ('scott', 324),\n",
       " ('realize', 324),\n",
       " ('telling', 324),\n",
       " ('material', 324),\n",
       " ('singing', 323),\n",
       " ('dancing', 322),\n",
       " ('visual', 321),\n",
       " ('imagine', 321),\n",
       " ('adult', 321),\n",
       " ('kept', 320),\n",
       " ('office', 320),\n",
       " ('uses', 319),\n",
       " ('stunning', 318),\n",
       " ('pure', 318),\n",
       " ('wait', 318),\n",
       " ('seriously', 317),\n",
       " ('copy', 317),\n",
       " ('previous', 317),\n",
       " ('review', 317),\n",
       " ('somehow', 316),\n",
       " ('reading', 316),\n",
       " ('create', 316),\n",
       " ('magic', 316),\n",
       " ('created', 316),\n",
       " ('hot', 316),\n",
       " ('stay', 315),\n",
       " ('crazy', 315),\n",
       " ('frank', 315),\n",
       " ('escape', 315),\n",
       " ('air', 315),\n",
       " ('attempt', 315),\n",
       " ('hands', 314),\n",
       " ('filled', 313),\n",
       " ('surprisingly', 312),\n",
       " ('average', 312),\n",
       " ('expected', 312),\n",
       " ('complex', 311),\n",
       " ('successful', 310),\n",
       " ('studio', 310),\n",
       " ('quickly', 310),\n",
       " ('plus', 309),\n",
       " ('male', 309),\n",
       " ('co', 307),\n",
       " ('minute', 306),\n",
       " ('following', 306),\n",
       " ('exciting', 306),\n",
       " ('images', 306),\n",
       " ('casting', 306),\n",
       " ('e', 305),\n",
       " ('reasons', 305),\n",
       " ('german', 305),\n",
       " ('members', 305),\n",
       " ('themes', 305),\n",
       " ('follows', 305),\n",
       " ('touch', 304),\n",
       " ('cute', 304),\n",
       " ('free', 304),\n",
       " ('edge', 304),\n",
       " ('genius', 304),\n",
       " ('outside', 303),\n",
       " ('ok', 302),\n",
       " ('admit', 302),\n",
       " ('reviews', 302),\n",
       " ('younger', 302),\n",
       " ('master', 301),\n",
       " ('fighting', 301),\n",
       " ('odd', 301),\n",
       " ('thanks', 300),\n",
       " ('recent', 300),\n",
       " ('comment', 300),\n",
       " ('break', 300),\n",
       " ('apart', 299),\n",
       " ('begin', 298),\n",
       " ('lovely', 298),\n",
       " ('emotions', 298),\n",
       " ('party', 297),\n",
       " ('italian', 297),\n",
       " ('doctor', 297),\n",
       " ('sequel', 296),\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 548962),\n",
       " ('.', 167538),\n",
       " ('the', 163389),\n",
       " ('a', 79321),\n",
       " ('and', 74385),\n",
       " ('of', 69009),\n",
       " ('to', 68974),\n",
       " ('br', 52637),\n",
       " ('is', 50083),\n",
       " ('it', 48327),\n",
       " ('i', 46880),\n",
       " ('in', 43753),\n",
       " ('this', 40920),\n",
       " ('that', 37615),\n",
       " ('s', 31546),\n",
       " ('was', 26291),\n",
       " ('movie', 24965),\n",
       " ('for', 21927),\n",
       " ('but', 21781),\n",
       " ('with', 20878),\n",
       " ('as', 20625),\n",
       " ('t', 20361),\n",
       " ('film', 19218),\n",
       " ('you', 17549),\n",
       " ('on', 17192),\n",
       " ('not', 16354),\n",
       " ('have', 15144),\n",
       " ('are', 14623),\n",
       " ('be', 14541),\n",
       " ('he', 13856),\n",
       " ('one', 13134),\n",
       " ('they', 13011),\n",
       " ('\\n', 12500),\n",
       " ('at', 12279),\n",
       " ('his', 12147),\n",
       " ('all', 12036),\n",
       " ('so', 11463),\n",
       " ('like', 11238),\n",
       " ('there', 10775),\n",
       " ('just', 10619),\n",
       " ('by', 10549),\n",
       " ('or', 10272),\n",
       " ('an', 10266),\n",
       " ('who', 9969),\n",
       " ('from', 9731),\n",
       " ('if', 9518),\n",
       " ('about', 9061),\n",
       " ('out', 8979),\n",
       " ('what', 8422),\n",
       " ('some', 8306),\n",
       " ('no', 8143),\n",
       " ('her', 7947),\n",
       " ('even', 7687),\n",
       " ('can', 7653),\n",
       " ('has', 7604),\n",
       " ('good', 7423),\n",
       " ('bad', 7401),\n",
       " ('would', 7036),\n",
       " ('up', 6970),\n",
       " ('only', 6781),\n",
       " ('more', 6730),\n",
       " ('when', 6726),\n",
       " ('she', 6444),\n",
       " ('really', 6262),\n",
       " ('time', 6209),\n",
       " ('had', 6142),\n",
       " ('my', 6015),\n",
       " ('were', 6001),\n",
       " ('which', 5780),\n",
       " ('very', 5764),\n",
       " ('me', 5606),\n",
       " ('see', 5452),\n",
       " ('don', 5336),\n",
       " ('we', 5328),\n",
       " ('their', 5278),\n",
       " ('do', 5236),\n",
       " ('story', 5208),\n",
       " ('than', 5183),\n",
       " ('been', 5100),\n",
       " ('much', 5078),\n",
       " ('get', 5037),\n",
       " ('because', 4966),\n",
       " ('people', 4806),\n",
       " ('then', 4761),\n",
       " ('make', 4722),\n",
       " ('how', 4688),\n",
       " ('could', 4686),\n",
       " ('any', 4658),\n",
       " ('into', 4567),\n",
       " ('made', 4541),\n",
       " ('first', 4306),\n",
       " ('other', 4305),\n",
       " ('well', 4254),\n",
       " ('too', 4174),\n",
       " ('them', 4165),\n",
       " ('plot', 4154),\n",
       " ('movies', 4080),\n",
       " ('acting', 4056),\n",
       " ('will', 3993),\n",
       " ('way', 3989),\n",
       " ('most', 3919),\n",
       " ('him', 3858),\n",
       " ('after', 3838),\n",
       " ('its', 3655),\n",
       " ('think', 3643),\n",
       " ('also', 3608),\n",
       " ('characters', 3600),\n",
       " ('off', 3567),\n",
       " ('watch', 3550),\n",
       " ('character', 3506),\n",
       " ('did', 3506),\n",
       " ('why', 3463),\n",
       " ('being', 3393),\n",
       " ('better', 3358),\n",
       " ('know', 3334),\n",
       " ('over', 3316),\n",
       " ('seen', 3265),\n",
       " ('ever', 3263),\n",
       " ('never', 3259),\n",
       " ('your', 3233),\n",
       " ('where', 3219),\n",
       " ('two', 3173),\n",
       " ('little', 3096),\n",
       " ('films', 3077),\n",
       " ('here', 3027),\n",
       " ('m', 3000),\n",
       " ('nothing', 2990),\n",
       " ('say', 2982),\n",
       " ('end', 2954),\n",
       " ('something', 2942),\n",
       " ('should', 2920),\n",
       " ('many', 2909),\n",
       " ('does', 2871),\n",
       " ('thing', 2866),\n",
       " ('show', 2862),\n",
       " ('ve', 2829),\n",
       " ('scene', 2816),\n",
       " ('scenes', 2785),\n",
       " ('these', 2724),\n",
       " ('go', 2717),\n",
       " ('didn', 2646),\n",
       " ('watching', 2640),\n",
       " ('great', 2640),\n",
       " ('re', 2620),\n",
       " ('doesn', 2601),\n",
       " ('through', 2560),\n",
       " ('such', 2544),\n",
       " ('man', 2516),\n",
       " ('worst', 2480),\n",
       " ('actually', 2449),\n",
       " ('actors', 2437),\n",
       " ('life', 2429),\n",
       " ('back', 2424),\n",
       " ('while', 2418),\n",
       " ('director', 2405),\n",
       " ('funny', 2336),\n",
       " ('going', 2319),\n",
       " ('still', 2283),\n",
       " ('another', 2254),\n",
       " ('look', 2247),\n",
       " ('now', 2237),\n",
       " ('old', 2215),\n",
       " ('those', 2212),\n",
       " ('real', 2170),\n",
       " ('few', 2158),\n",
       " ('love', 2152),\n",
       " ('horror', 2150),\n",
       " ('before', 2147),\n",
       " ('want', 2141),\n",
       " ('minutes', 2126),\n",
       " ('pretty', 2115),\n",
       " ('best', 2094),\n",
       " ('though', 2091),\n",
       " ('same', 2081),\n",
       " ('script', 2074),\n",
       " ('work', 2027),\n",
       " ('every', 2025),\n",
       " ('seems', 2023),\n",
       " ('least', 2011),\n",
       " ('enough', 1997),\n",
       " ('down', 1988),\n",
       " ('original', 1983),\n",
       " ('guy', 1964),\n",
       " ('got', 1952),\n",
       " ('around', 1943),\n",
       " ('part', 1942),\n",
       " ('lot', 1892),\n",
       " ('anything', 1874),\n",
       " ('find', 1860),\n",
       " ('new', 1854),\n",
       " ('again', 1849),\n",
       " ('isn', 1849),\n",
       " ('point', 1845),\n",
       " ('fact', 1839),\n",
       " ('things', 1839),\n",
       " ('give', 1823),\n",
       " ('makes', 1814),\n",
       " ('take', 1800),\n",
       " ('thought', 1798),\n",
       " ('d', 1770),\n",
       " ('whole', 1768),\n",
       " ('long', 1761),\n",
       " ('years', 1759),\n",
       " ('however', 1740),\n",
       " ('gets', 1714),\n",
       " ('making', 1695),\n",
       " ('cast', 1694),\n",
       " ('big', 1662),\n",
       " ('might', 1658),\n",
       " ('interesting', 1648),\n",
       " ('money', 1638),\n",
       " ('us', 1628),\n",
       " ('right', 1625),\n",
       " ('far', 1619),\n",
       " ('quite', 1596),\n",
       " ('without', 1595),\n",
       " ('come', 1595),\n",
       " ('almost', 1574),\n",
       " ('ll', 1567),\n",
       " ('action', 1566),\n",
       " ('awful', 1557),\n",
       " ('kind', 1539),\n",
       " ('reason', 1534),\n",
       " ('am', 1530),\n",
       " ('looks', 1528),\n",
       " ('must', 1522),\n",
       " ('done', 1510),\n",
       " ('comedy', 1504),\n",
       " ('someone', 1490),\n",
       " ('trying', 1486),\n",
       " ('wasn', 1484),\n",
       " ('poor', 1481),\n",
       " ('boring', 1478),\n",
       " ('instead', 1478),\n",
       " ('saw', 1475),\n",
       " ('away', 1469),\n",
       " ('girl', 1463),\n",
       " ('probably', 1444),\n",
       " ('believe', 1434),\n",
       " ('sure', 1433),\n",
       " ('looking', 1430),\n",
       " ('stupid', 1428),\n",
       " ('anyone', 1418),\n",
       " ('times', 1406),\n",
       " ('world', 1404),\n",
       " ('maybe', 1404),\n",
       " ('rather', 1394),\n",
       " ('terrible', 1391),\n",
       " ('last', 1390),\n",
       " ('may', 1390),\n",
       " ('since', 1388),\n",
       " ('let', 1385),\n",
       " ('tv', 1382),\n",
       " ('between', 1374),\n",
       " ('hard', 1374),\n",
       " ('waste', 1358),\n",
       " ('woman', 1356),\n",
       " ('feel', 1354),\n",
       " ('effects', 1348),\n",
       " ('half', 1341),\n",
       " ('own', 1333),\n",
       " ('young', 1317),\n",
       " ('music', 1316),\n",
       " ('idea', 1312),\n",
       " ('sense', 1306),\n",
       " ('bit', 1298),\n",
       " ('having', 1280),\n",
       " ('book', 1278),\n",
       " ('found', 1267),\n",
       " ('put', 1263),\n",
       " ('series', 1263),\n",
       " ('goes', 1256),\n",
       " ('worse', 1249),\n",
       " ('said', 1230),\n",
       " ('comes', 1224),\n",
       " ('role', 1222),\n",
       " ('main', 1220),\n",
       " ('else', 1199),\n",
       " ('everything', 1197),\n",
       " ('yet', 1196),\n",
       " ('low', 1189),\n",
       " ('screen', 1188),\n",
       " ('supposed', 1186),\n",
       " ('actor', 1185),\n",
       " ('either', 1183),\n",
       " ('budget', 1179),\n",
       " ('ending', 1179),\n",
       " ('audience', 1178),\n",
       " ('set', 1177),\n",
       " ('family', 1170),\n",
       " ('left', 1169),\n",
       " ('completely', 1168),\n",
       " ('both', 1158),\n",
       " ('wrong', 1155),\n",
       " ('always', 1151),\n",
       " ('place', 1148),\n",
       " ('course', 1148),\n",
       " ('seem', 1147),\n",
       " ('watched', 1142),\n",
       " ('day', 1132),\n",
       " ('simply', 1130),\n",
       " ('shot', 1126),\n",
       " ('mean', 1117),\n",
       " ('special', 1102),\n",
       " ('dead', 1101),\n",
       " ('three', 1094),\n",
       " ('house', 1085),\n",
       " ('oh', 1084),\n",
       " ('night', 1083),\n",
       " ('read', 1082),\n",
       " ('less', 1067),\n",
       " ('high', 1066),\n",
       " ('year', 1064),\n",
       " ('camera', 1061),\n",
       " ('worth', 1057),\n",
       " ('our', 1056),\n",
       " ('try', 1051),\n",
       " ('horrible', 1046),\n",
       " ('sex', 1046),\n",
       " ('video', 1043),\n",
       " ('black', 1039),\n",
       " ('although', 1036),\n",
       " ('couldn', 1036),\n",
       " ('once', 1033),\n",
       " ('rest', 1022),\n",
       " ('dvd', 1021),\n",
       " ('line', 1018),\n",
       " ('played', 1017),\n",
       " ('fun', 1007),\n",
       " ('during', 1006),\n",
       " ('production', 1003),\n",
       " ('everyone', 1002),\n",
       " ('play', 993),\n",
       " ('mind', 990),\n",
       " ('kids', 989),\n",
       " ('version', 989),\n",
       " ('seeing', 988),\n",
       " ('american', 980),\n",
       " ('given', 978),\n",
       " ('used', 969),\n",
       " ('performance', 968),\n",
       " ('together', 963),\n",
       " ('especially', 963),\n",
       " ('tell', 959),\n",
       " ('women', 958),\n",
       " ('start', 956),\n",
       " ('need', 955),\n",
       " ('second', 953),\n",
       " ('takes', 950),\n",
       " ('each', 950),\n",
       " ('wife', 944),\n",
       " ('dialogue', 942),\n",
       " ('use', 940),\n",
       " ('problem', 938),\n",
       " ('star', 934),\n",
       " ('unfortunately', 931),\n",
       " ('himself', 929),\n",
       " ('doing', 926),\n",
       " ('death', 922),\n",
       " ('name', 921),\n",
       " ('lines', 919),\n",
       " ('killer', 914),\n",
       " ('getting', 913),\n",
       " ('help', 905),\n",
       " ('couple', 902),\n",
       " ('fan', 902),\n",
       " ('head', 898),\n",
       " ('crap', 895),\n",
       " ('guess', 888),\n",
       " ('piece', 884),\n",
       " ('nice', 880),\n",
       " ('different', 878),\n",
       " ('school', 876),\n",
       " ('later', 875),\n",
       " ('entire', 869),\n",
       " ('shows', 860),\n",
       " ('next', 858),\n",
       " ('john', 858),\n",
       " ('short', 857),\n",
       " ('seemed', 857),\n",
       " ('hollywood', 850),\n",
       " ('home', 848),\n",
       " ('person', 846),\n",
       " ('true', 846),\n",
       " ('absolutely', 842),\n",
       " ('sort', 840),\n",
       " ('care', 839),\n",
       " ('understand', 836),\n",
       " ('plays', 835),\n",
       " ('felt', 834),\n",
       " ('written', 829),\n",
       " ('title', 828),\n",
       " ('men', 822),\n",
       " ('until', 821),\n",
       " ('flick', 816),\n",
       " ('decent', 815),\n",
       " ('face', 814),\n",
       " ('friends', 810),\n",
       " ('stars', 807),\n",
       " ('case', 807),\n",
       " ('job', 807),\n",
       " ('itself', 804),\n",
       " ('yes', 801),\n",
       " ('perhaps', 800),\n",
       " ('wanted', 797),\n",
       " ('went', 797),\n",
       " ('called', 796),\n",
       " ('annoying', 795),\n",
       " ('ridiculous', 790),\n",
       " ('tries', 790),\n",
       " ('laugh', 788),\n",
       " ('evil', 787),\n",
       " ('along', 786),\n",
       " ('top', 785),\n",
       " ('hour', 784),\n",
       " ('full', 783),\n",
       " ('came', 780),\n",
       " ('writing', 780),\n",
       " ('keep', 770),\n",
       " ('totally', 767),\n",
       " ('playing', 766),\n",
       " ('god', 765),\n",
       " ('won', 764),\n",
       " ('guys', 763),\n",
       " ('already', 762),\n",
       " ('gore', 757),\n",
       " ('direction', 748),\n",
       " ('save', 746),\n",
       " ('lost', 745),\n",
       " ('example', 744),\n",
       " ('sound', 742),\n",
       " ('war', 741),\n",
       " ('attempt', 735),\n",
       " ('except', 733),\n",
       " ('car', 733),\n",
       " ('moments', 732),\n",
       " ('blood', 732),\n",
       " ('obviously', 730),\n",
       " ('act', 729),\n",
       " ('remember', 728),\n",
       " ('kill', 727),\n",
       " ('father', 726),\n",
       " ('truly', 726),\n",
       " ('white', 726),\n",
       " ('b', 725),\n",
       " ('thinking', 720),\n",
       " ('ok', 716),\n",
       " ('finally', 716),\n",
       " ('turn', 711),\n",
       " ('quality', 701),\n",
       " ('lack', 698),\n",
       " ('style', 694),\n",
       " ('wouldn', 693),\n",
       " ('cheap', 691),\n",
       " ('none', 690),\n",
       " ('kid', 686),\n",
       " ('please', 686),\n",
       " ('boy', 685),\n",
       " ('seriously', 684),\n",
       " ('lead', 680),\n",
       " ('dull', 677),\n",
       " ('children', 676),\n",
       " ('starts', 675),\n",
       " ('stuff', 673),\n",
       " ('hope', 672),\n",
       " ('looked', 670),\n",
       " ('recommend', 669),\n",
       " ('under', 668),\n",
       " ('run', 667),\n",
       " ('killed', 667),\n",
       " ('enjoy', 666),\n",
       " ('others', 666),\n",
       " ('myself', 663),\n",
       " ('etc', 663),\n",
       " ('beginning', 662),\n",
       " ('girls', 662),\n",
       " ('against', 662),\n",
       " ('small', 660),\n",
       " ('obvious', 660),\n",
       " ('hell', 659),\n",
       " ('slow', 657),\n",
       " ('hand', 656),\n",
       " ('wonder', 652),\n",
       " ('lame', 652),\n",
       " ('picture', 651),\n",
       " ('becomes', 651),\n",
       " ('based', 650),\n",
       " ('early', 648),\n",
       " ('behind', 646),\n",
       " ('poorly', 644),\n",
       " ('avoid', 642),\n",
       " ('apparently', 640),\n",
       " ('complete', 640),\n",
       " ('happens', 639),\n",
       " ('anyway', 638),\n",
       " ('classic', 637),\n",
       " ('several', 636),\n",
       " ('despite', 635),\n",
       " ('episode', 635),\n",
       " ('certainly', 635),\n",
       " ('often', 631),\n",
       " ('cut', 630),\n",
       " ('writer', 630),\n",
       " ('gave', 628),\n",
       " ('predictable', 628),\n",
       " ('mother', 628),\n",
       " ('become', 627),\n",
       " ('close', 625),\n",
       " ('fans', 624),\n",
       " ('saying', 621),\n",
       " ('scary', 619),\n",
       " ('live', 618),\n",
       " ('stop', 618),\n",
       " ('wants', 617),\n",
       " ('self', 615),\n",
       " ('mr', 612),\n",
       " ('jokes', 611),\n",
       " ('friend', 611),\n",
       " ('cannot', 610),\n",
       " ('overall', 609),\n",
       " ('cinema', 604),\n",
       " ('child', 603),\n",
       " ('silly', 601),\n",
       " ('beautiful', 596),\n",
       " ('human', 595),\n",
       " ('expect', 594),\n",
       " ('liked', 593),\n",
       " ('happened', 592),\n",
       " ('entertaining', 590),\n",
       " ('bunch', 590),\n",
       " ('actress', 588),\n",
       " ('final', 588),\n",
       " ('says', 584),\n",
       " ('performances', 584),\n",
       " ('turns', 577),\n",
       " ('humor', 577),\n",
       " ('themselves', 576),\n",
       " ('eyes', 576),\n",
       " ('hours', 574),\n",
       " ('happen', 573),\n",
       " ('days', 572),\n",
       " ('basically', 572),\n",
       " ('running', 571),\n",
       " ('involved', 569),\n",
       " ('call', 569),\n",
       " ('disappointed', 569),\n",
       " ('group', 568),\n",
       " ('directed', 568),\n",
       " ('fight', 567),\n",
       " ('talking', 566),\n",
       " ('daughter', 566),\n",
       " ('body', 566),\n",
       " ('sorry', 565),\n",
       " ('badly', 565),\n",
       " ('throughout', 563),\n",
       " ('viewer', 563),\n",
       " ('extremely', 562),\n",
       " ('yourself', 562),\n",
       " ('heard', 561),\n",
       " ('interest', 561),\n",
       " ('violence', 561),\n",
       " ('shots', 559),\n",
       " ('side', 557),\n",
       " ('word', 556),\n",
       " ('art', 555),\n",
       " ('possible', 554),\n",
       " ('dark', 551),\n",
       " ('game', 551),\n",
       " ('hero', 550),\n",
       " ('alone', 549),\n",
       " ('type', 547),\n",
       " ('son', 547),\n",
       " ('leave', 547),\n",
       " ('parts', 546),\n",
       " ('single', 546),\n",
       " ('gives', 546),\n",
       " ('started', 545),\n",
       " ('female', 543),\n",
       " ('voice', 541),\n",
       " ('mess', 541),\n",
       " ('rating', 541),\n",
       " ('town', 540),\n",
       " ('aren', 540),\n",
       " ('drama', 538),\n",
       " ('definitely', 537),\n",
       " ('unless', 536),\n",
       " ('review', 534),\n",
       " ('effort', 533),\n",
       " ('weak', 533),\n",
       " ('able', 533),\n",
       " ('took', 531),\n",
       " ('non', 530),\n",
       " ('five', 530),\n",
       " ('matter', 529),\n",
       " ('usually', 529),\n",
       " ('michael', 528),\n",
       " ('feeling', 526),\n",
       " ('huge', 523),\n",
       " ('sequel', 522),\n",
       " ('soon', 521),\n",
       " ('exactly', 520),\n",
       " ('past', 519),\n",
       " ('turned', 518),\n",
       " ('police', 518),\n",
       " ('tried', 515),\n",
       " ('talent', 513),\n",
       " ('middle', 513),\n",
       " ('genre', 512),\n",
       " ('zombie', 510),\n",
       " ('ends', 509),\n",
       " ('history', 509),\n",
       " ('straight', 503),\n",
       " ('coming', 501),\n",
       " ('opening', 501),\n",
       " ('serious', 501),\n",
       " ('moment', 500),\n",
       " ('lives', 499),\n",
       " ('sad', 499),\n",
       " ('dialog', 498),\n",
       " ('particularly', 498),\n",
       " ('editing', 493),\n",
       " ('clearly', 492),\n",
       " ('beyond', 491),\n",
       " ('earth', 491),\n",
       " ('taken', 490),\n",
       " ('cool', 490),\n",
       " ('level', 489),\n",
       " ('dumb', 489),\n",
       " ('okay', 488),\n",
       " ('major', 487),\n",
       " ('premise', 485),\n",
       " ('fast', 485),\n",
       " ('joke', 484),\n",
       " ('stories', 484),\n",
       " ('minute', 483),\n",
       " ('wasted', 483),\n",
       " ('across', 482),\n",
       " ('mostly', 482),\n",
       " ('rent', 482),\n",
       " ('late', 481),\n",
       " ('falls', 481),\n",
       " ('fails', 481),\n",
       " ('mention', 478),\n",
       " ('theater', 475),\n",
       " ('sometimes', 472),\n",
       " ('stay', 472),\n",
       " ('hit', 468),\n",
       " ('talk', 467),\n",
       " ('fine', 467),\n",
       " ('die', 466),\n",
       " ('pointless', 465),\n",
       " ('storyline', 465),\n",
       " ('taking', 464),\n",
       " ('order', 462),\n",
       " ('brother', 461),\n",
       " ('told', 460),\n",
       " ('whatever', 460),\n",
       " ('wish', 458),\n",
       " ('room', 456),\n",
       " ('appears', 455),\n",
       " ('write', 455),\n",
       " ('career', 455),\n",
       " ('husband', 454),\n",
       " ('known', 454),\n",
       " ('living', 451),\n",
       " ('ten', 450),\n",
       " ('sit', 450),\n",
       " ('words', 449),\n",
       " ('monster', 448),\n",
       " ('chance', 448),\n",
       " ('novel', 444),\n",
       " ('hate', 444),\n",
       " ('add', 443),\n",
       " ('english', 443),\n",
       " ('somehow', 441),\n",
       " ('strange', 440),\n",
       " ('actual', 438),\n",
       " ('imdb', 438),\n",
       " ('killing', 437),\n",
       " ('total', 437),\n",
       " ('material', 437),\n",
       " ('ones', 437),\n",
       " ('knew', 436),\n",
       " ('king', 434),\n",
       " ('number', 434),\n",
       " ('using', 433),\n",
       " ('lee', 431),\n",
       " ('shown', 431),\n",
       " ('giving', 431),\n",
       " ('works', 431),\n",
       " ('power', 431),\n",
       " ('possibly', 430),\n",
       " ('kept', 430),\n",
       " ('points', 430),\n",
       " ('four', 429),\n",
       " ('local', 427),\n",
       " ('usual', 426),\n",
       " ('including', 425),\n",
       " ('problems', 424),\n",
       " ('ago', 424),\n",
       " ('opinion', 424),\n",
       " ('nudity', 423),\n",
       " ('age', 422),\n",
       " ('due', 421),\n",
       " ('roles', 420),\n",
       " ('writers', 419),\n",
       " ('decided', 419),\n",
       " ('flat', 418),\n",
       " ('near', 418),\n",
       " ('easily', 418),\n",
       " ('experience', 417),\n",
       " ('murder', 417),\n",
       " ('reviews', 416),\n",
       " ('imagine', 415),\n",
       " ('feels', 413),\n",
       " ('somewhat', 411),\n",
       " ('plain', 411),\n",
       " ('score', 410),\n",
       " ('class', 410),\n",
       " ('bring', 409),\n",
       " ('whether', 409),\n",
       " ('song', 409),\n",
       " ('average', 408),\n",
       " ('whose', 408),\n",
       " ('otherwise', 408),\n",
       " ('zombies', 407),\n",
       " ('pathetic', 407),\n",
       " ('nearly', 407),\n",
       " ('knows', 407),\n",
       " ('cinematography', 406),\n",
       " ('upon', 406),\n",
       " ('cheesy', 406),\n",
       " ('space', 405),\n",
       " ('city', 405),\n",
       " ('credits', 404),\n",
       " ('lots', 403),\n",
       " ('change', 403),\n",
       " ('james', 403),\n",
       " ('nor', 402),\n",
       " ('entertainment', 402),\n",
       " ('wait', 401),\n",
       " ('released', 400),\n",
       " ('needs', 399),\n",
       " ('shame', 398),\n",
       " ('attention', 396),\n",
       " ('comments', 394),\n",
       " ('lady', 393),\n",
       " ('bored', 393),\n",
       " ('free', 393),\n",
       " ('needed', 392),\n",
       " ('expected', 392),\n",
       " ('clear', 392),\n",
       " ('view', 391),\n",
       " ('development', 390),\n",
       " ('doubt', 390),\n",
       " ('check', 390),\n",
       " ('figure', 389),\n",
       " ('mystery', 389),\n",
       " ('excellent', 388),\n",
       " ('garbage', 388),\n",
       " ('television', 386),\n",
       " ('sequence', 386),\n",
       " ('o', 385),\n",
       " ('sets', 385),\n",
       " ('potential', 384),\n",
       " ('laughable', 384),\n",
       " ('documentary', 382),\n",
       " ('robert', 382),\n",
       " ('country', 382),\n",
       " ('light', 382),\n",
       " ('reality', 382),\n",
       " ('ask', 381),\n",
       " ('general', 381),\n",
       " ('fall', 380),\n",
       " ('comic', 380),\n",
       " ('begin', 380),\n",
       " ('footage', 379),\n",
       " ('stand', 379),\n",
       " ('forced', 379),\n",
       " ('remake', 379),\n",
       " ('trash', 379),\n",
       " ('songs', 378),\n",
       " ('thriller', 378),\n",
       " ('within', 377),\n",
       " ('gay', 377),\n",
       " ('hardly', 376),\n",
       " ('gone', 375),\n",
       " ('above', 375),\n",
       " ('george', 374),\n",
       " ('means', 373),\n",
       " ('sounds', 373),\n",
       " ('david', 372),\n",
       " ('buy', 372),\n",
       " ('move', 372),\n",
       " ('directing', 372),\n",
       " ('forward', 371),\n",
       " ('rock', 371),\n",
       " ('important', 371),\n",
       " ('filmed', 370),\n",
       " ('british', 370),\n",
       " ('hot', 370),\n",
       " ('haven', 370),\n",
       " ('reading', 369),\n",
       " ('fake', 369),\n",
       " ('heart', 369),\n",
       " ('incredibly', 368),\n",
       " ('hear', 368),\n",
       " ('weird', 368),\n",
       " ('musical', 367),\n",
       " ('cop', 367),\n",
       " ('hilarious', 367),\n",
       " ('enjoyed', 367),\n",
       " ('happy', 366),\n",
       " ('message', 366),\n",
       " ('pay', 366),\n",
       " ('box', 365),\n",
       " ('laughs', 365),\n",
       " ('suspense', 363),\n",
       " ('sadly', 363),\n",
       " ('eye', 362),\n",
       " ('similar', 361),\n",
       " ('third', 361),\n",
       " ('named', 361),\n",
       " ('modern', 360),\n",
       " ('failed', 359),\n",
       " ('events', 359),\n",
       " ('question', 358),\n",
       " ('forget', 358),\n",
       " ('male', 357),\n",
       " ('finds', 357),\n",
       " ('perfect', 356),\n",
       " ('sister', 355),\n",
       " ('spent', 355),\n",
       " ('feature', 354),\n",
       " ('result', 354),\n",
       " ('comment', 353),\n",
       " ('girlfriend', 353),\n",
       " ('sexual', 352),\n",
       " ('neither', 351),\n",
       " ('richard', 351),\n",
       " ('attempts', 351),\n",
       " ('elements', 350),\n",
       " ('screenplay', 350),\n",
       " ('spoilers', 349),\n",
       " ('showing', 348),\n",
       " ('filmmakers', 348),\n",
       " ('brain', 348),\n",
       " ('miss', 347),\n",
       " ('dr', 347),\n",
       " ('christmas', 347),\n",
       " ('cover', 345),\n",
       " ('red', 344),\n",
       " ('sequences', 344),\n",
       " ('excuse', 343),\n",
       " ('typical', 343),\n",
       " ('baby', 342),\n",
       " ('crazy', 342),\n",
       " ('ideas', 342),\n",
       " ('meant', 341),\n",
       " ('loved', 341),\n",
       " ('fire', 340),\n",
       " ('worked', 340),\n",
       " ('unbelievable', 339),\n",
       " ('follow', 339),\n",
       " ('theme', 337),\n",
       " ('twist', 336),\n",
       " ('producers', 336),\n",
       " ('appear', 336),\n",
       " ('plus', 336),\n",
       " ('barely', 336),\n",
       " ('directors', 335),\n",
       " ('team', 335),\n",
       " ('viewers', 333),\n",
       " ('slasher', 332),\n",
       " ('tom', 332),\n",
       " ('leads', 332),\n",
       " ('working', 331),\n",
       " ('gun', 331),\n",
       " ('wrote', 331),\n",
       " ('villain', 331),\n",
       " ('realize', 330),\n",
       " ('strong', 330),\n",
       " ('island', 330),\n",
       " ('open', 330),\n",
       " ('yeah', 329),\n",
       " ('positive', 329),\n",
       " ('quickly', 329),\n",
       " ('disappointing', 329),\n",
       " ('simple', 328),\n",
       " ('honestly', 328),\n",
       " ('release', 328),\n",
       " ('weren', 328),\n",
       " ('tells', 327),\n",
       " ('period', 327),\n",
       " ('kills', 327),\n",
       " ('eventually', 327),\n",
       " ('doctor', 327),\n",
       " ('acted', 326),\n",
       " ('list', 326),\n",
       " ('herself', 326),\n",
       " ('dog', 326),\n",
       " ('nowhere', 326),\n",
       " ('walk', 325),\n",
       " ('apart', 324),\n",
       " ('air', 324),\n",
       " ('makers', 323),\n",
       " ('subject', 323),\n",
       " ('learn', 322),\n",
       " ('fi', 322),\n",
       " ('sci', 319),\n",
       " ('bother', 319),\n",
       " ('admit', 319),\n",
       " ('disappointment', 318),\n",
       " ('hands', 318),\n",
       " ('jack', 318),\n",
       " ('note', 318),\n",
       " ('e', 317),\n",
       " ('value', 317),\n",
       " ('certain', 317),\n",
       " ('casting', 317),\n",
       " ('peter', 316),\n",
       " ('grade', 316),\n",
       " ('missing', 315),\n",
       " ('suddenly', 315),\n",
       " ('form', 313),\n",
       " ('previous', 313),\n",
       " ('break', 313),\n",
       " ('stick', 313),\n",
       " ('soundtrack', 312),\n",
       " ('surprised', 311),\n",
       " ('expecting', 311),\n",
       " ('front', 311),\n",
       " ('parents', 310),\n",
       " ('relationship', 310),\n",
       " ('surprise', 310),\n",
       " ('today', 309),\n",
       " ('shoot', 309),\n",
       " ('concept', 308),\n",
       " ('ended', 308),\n",
       " ('vampire', 308),\n",
       " ('leaves', 308),\n",
       " ('painful', 308),\n",
       " ('somewhere', 308),\n",
       " ('creepy', 308),\n",
       " ('ways', 308),\n",
       " ('spend', 307),\n",
       " ('th', 307),\n",
       " ('difficult', 306),\n",
       " ('future', 306),\n",
       " ('fighting', 306),\n",
       " ('street', 306),\n",
       " ('effect', 306),\n",
       " ('america', 305),\n",
       " ('c', 305),\n",
       " ('accent', 304),\n",
       " ('project', 302),\n",
       " ('truth', 302),\n",
       " ('deal', 301),\n",
       " ('joe', 301),\n",
       " ('indeed', 301),\n",
       " ('f', 301),\n",
       " ('rate', 300),\n",
       " ('biggest', 300),\n",
       " ('paul', 299),\n",
       " ('japanese', 299),\n",
       " ('begins', 298),\n",
       " ('utterly', 298),\n",
       " ('redeeming', 298),\n",
       " ('college', 298),\n",
       " ('fairly', 297),\n",
       " ('disney', 297),\n",
       " ('york', 297),\n",
       " ('crew', 296),\n",
       " ('revenge', 296),\n",
       " ('create', 296),\n",
       " ('cartoon', 296),\n",
       " ('co', 295),\n",
       " ('stage', 295),\n",
       " ('interested', 295),\n",
       " ('outside', 295),\n",
       " ('computer', 295),\n",
       " ('among', 294),\n",
       " ('considering', 294),\n",
       " ('speak', 294),\n",
       " ('towards', 293),\n",
       " ('channel', 293),\n",
       " ('sick', 293),\n",
       " ('cause', 292),\n",
       " ('hair', 292),\n",
       " ('talented', 292),\n",
       " ('van', 292),\n",
       " ('particular', 292),\n",
       " ('bottom', 291),\n",
       " ('reasons', 291),\n",
       " ('telling', 290),\n",
       " ('mediocre', 290),\n",
       " ('cat', 290),\n",
       " ('store', 289),\n",
       " ('supporting', 289),\n",
       " ('hoping', 288),\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_counts.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is just to show the most common words in the positive and negative sentences. However, there are a lot of unnecessary words like `the`, `a`, `was`, and so on. Can you find a way to show the relevant words and not these words? \n",
    "\n",
    "```\n",
    "Hint: Stop Words removal or normalizing each term.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell',\n",
       " 'high',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cartoon',\n",
       " 'comedy',\n",
       " 'it',\n",
       " 'ran',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'as',\n",
       " 'some',\n",
       " 'other',\n",
       " 'programs',\n",
       " 'about',\n",
       " 'school',\n",
       " 'life',\n",
       " 'such',\n",
       " 'as',\n",
       " 'teachers',\n",
       " 'my',\n",
       " 'years',\n",
       " 'in',\n",
       " 'the',\n",
       " 'teaching',\n",
       " 'profession',\n",
       " 'lead',\n",
       " 'me']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43732,\n",
       " 59198,\n",
       " 28537,\n",
       " 62650,\n",
       " 52828,\n",
       " 3699,\n",
       " 28540,\n",
       " 69932,\n",
       " 20610,\n",
       " 23859,\n",
       " 29728,\n",
       " 56652,\n",
       " 20607,\n",
       " 63343,\n",
       " 36918,\n",
       " 9940,\n",
       " 50681,\n",
       " 9415,\n",
       " 19130,\n",
       " 56915,\n",
       " 20607,\n",
       " 33161,\n",
       " 32588,\n",
       " 1442,\n",
       " 62078,\n",
       " 23859,\n",
       " 64734,\n",
       " 5599,\n",
       " 73870,\n",
       " 32565]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab_to_int[word] for word in words[:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43732"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int['bromwell']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding\n",
    "\n",
    "We need one hot encoding for the labels. Think of a reason why we need one hot encoded labels for classes?\n",
    "\n",
    "## Task 3: Create one hot encoding for the labels. \n",
    "\n",
    "* Write the one hot encoding logic in the `one_hot` function.\n",
    "* Use 1 for positive label and 0 for negative label.\n",
    "* Save all the values in the `encoded_labels` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 for positive label and 0 for negative label\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "def one_hot(labels):\n",
    "    encoded_labels = np.asarray(labels)    \n",
    "    condlist = [encoded_labels=='positive\\n', encoded_labels=='negative\\n']\n",
    "    choicelist = [1, 0]\n",
    "    one_hot = np.select(condlist, choicelist)\n",
    "    return one_hot\n",
    "    \n",
    " \n",
    "encoded_labels = one_hot(labels)\n",
    "encoded_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the length of your label and uncomment next line only if the encoded_labels size is 25001.\n",
    "# If you dont get the intuition behind this step, print encoded_labels to see it.\n",
    "#encoded_labels = encoded_labels[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_ints: list like reviews_split but containing corresponding integer instead of word. contains 25000 reviews\n",
    "reviews_ints = []\n",
    "for review in reviews_split:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in review.split()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 0\n",
      "Maximum review length: 2514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This step is to see if any review is empty and we remove it. Otherwise the input will be all zeroes.\n",
    "# review_lens: how many similar length reviews occur and length of reviews\n",
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))\n",
    "review_lens[2514]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of reviews before removing outliers: ', 25000)\n",
      "('Number of reviews after removing outliers: ', 25000)\n"
     ]
    }
   ],
   "source": [
    "print('Number of reviews before removing outliers: ', len(reviews_ints))\n",
    "\n",
    "## remove any reviews/labels with zero length from the reviews_ints list.\n",
    "\n",
    "# get indices of any reviews with length 0\n",
    "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
    "\n",
    "# remove 0-length reviews and their labels\n",
    "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
    "\n",
    "print('Number of reviews after removing outliers: ', len(reviews_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Padding the data\n",
    "\n",
    "> Define a function that returns an array `features` that contains the padded data, of a standard size, that we'll pass to the network. \n",
    "* The data should come from `review_ints`, since we want to feed integers to the network. \n",
    "* Each row should be `seq_length` elements long. \n",
    "* For reviews shorter than `seq_length` words, **left pad** with 0s. That is, if the review is `['best', 'movie', 'ever']`, `[117, 18, 128]` as integers, the row will look like `[0, 0, 0, ..., 0, 117, 18, 128]`. \n",
    "* For reviews longer than `seq_length`, use only the first `seq_length` words as the feature vector.\n",
    "\n",
    "As a small example, if the `seq_length=10` and an input review is: \n",
    "```\n",
    "[117, 18, 128]\n",
    "```\n",
    "The resultant, padded sequence should be: \n",
    "\n",
    "```\n",
    "[0, 0, 0, 0, 0, 0, 0, 117, 18, 128]\n",
    "```\n",
    "\n",
    "**Your final `features` array should be a 2D array, with as many rows as there are reviews, and as many columns as the specified `seq_length`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the logic for padding the data\n",
    "\n",
    "def pad_features(reviews_ints, seq_length):\n",
    "    padded = []\n",
    "    for review in reviews_ints:\n",
    "        \n",
    "        if (len(review) >= seq_length):\n",
    "            review = review[:seq_length]\n",
    "        else:\n",
    "            review = [0 for _ in range(seq_length-len(review))]+review[:]\n",
    "        padded.append(review)\n",
    "    \n",
    "    return np.asarray(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [54757 34566 45716 20607 71033 43935 20874   886 33120 20604]\n",
      " [30208 65137 20607 62650 69307 27812 42136 58102 28537 46955]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [14822 27396 42637 47823 32588  3213 32848 32565 56777 39300]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [23859 55573 46491 21663 62650 27812 73293 28753 62078 59825]\n",
      " [62078 59205 21153 51903 55651 37505 45643 34541 42661 34367]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [23859  5513 41107 58633 57123  5294 18486 56421 68754 25482]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [62349 73817 58633 57123 32762  7224 63848 28537 37978 69357]\n",
      " [23859 48448 28537 65353 15938 28537 33474 36918 42633 39300]\n",
      " [14822 27396 10679 25095 25139  7577 16518 27396 42637 44110]\n",
      " [59205 48448 28537  1595 67660 20956 58558 59835  9270 28537]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [15938 72401 38809 43874  5528 62078 23859 46735 34549 47646]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [27396 42199 23859  5513 41107 25379   729 28540  1856  1595]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Verify if everything till now is correct. \n",
    "\n",
    "seq_length = 200\n",
    "\n",
    "features = pad_features(reviews_ints, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 10 values of the first 30 batches \n",
    "print(features[:30,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7, 8, 9], [1, 2, 3, 4], [5, 6]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "a = [[1, 2, 3, 4], [5, 6], [7, 8, 9]]\n",
    "# random.seed(101)\n",
    "random.shuffle(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything ready. It's time to split our dataset into `Train`, `Test` and `Validate`. \n",
    "\n",
    "Read more about the train-test-split here : https://cs230-stanford.github.io/train-dev-test-split.html\n",
    "\n",
    "## Task 5: Lets create train, test and val split in the ratio of 8:1:1.  \n",
    "\n",
    "Hint: Either use shuffle and slicing in Python or use train-test-val split in Sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import random\n",
    "\n",
    "train_frac = 0.8\n",
    "val_frac = 0.1\n",
    "test_frac = 0.1\n",
    "\n",
    "\n",
    "def train_test_val_split(features):\n",
    "    random.seed(101)\n",
    "    random.shuffle(features)\n",
    "    split_1 = int(0.8 * len(features))\n",
    "    split_2 = int(0.9 * len(features))\n",
    "    train_x = features[:split_1]\n",
    "    val_x = features[split_1:split_2]\n",
    "    test_x = features[split_2:]\n",
    "    return train_x, val_x, test_x\n",
    "\n",
    "def train_test_val_labels(encoded_labels):\n",
    "    random.seed(102)\n",
    "    random.shuffle(features)\n",
    "    split_1 = int(0.8 * len(encoded_labels))\n",
    "    split_2 = int(0.9 * len(encoded_labels))\n",
    "    train_y = encoded_labels[:split_1]\n",
    "    val_y = encoded_labels[split_1:split_2]\n",
    "    test_y = encoded_labels[split_2:]\n",
    "    return train_y, val_y, test_y\n",
    "\n",
    "train_x, val_x, test_x = train_test_val_split(features)\n",
    "train_y, val_y, test_y = train_test_val_labels(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "('Train set: \\t\\t(20000, 200)', '\\nValidation set: \\t(2500, 200)', '\\nTest set: \\t\\t(2500, 200)')\n"
     ]
    }
   ],
   "source": [
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders and Batching\n",
    "\n",
    "After creating training, test, and validation data, we can create DataLoaders for this data by following two steps:\n",
    "1. Create a known format for accessing our data, using [TensorDataset](https://pytorch.org/docs/stable/data.html#) which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset.\n",
    "2. Create DataLoaders and batch our training, validation, and test Tensor datasets.\n",
    "\n",
    "```\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "```\n",
    "\n",
    "This is an alternative to creating a generator function for batching our data into full batches.\n",
    "\n",
    "### Task 6: Create a generator function for the dataset. \n",
    "See the above link for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets for train, test and val\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50 \n",
    "\n",
    "# make sure to SHUFFLE your training data. Keep Shuffle=True.\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size,shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sample input size: ', torch.Size([50, 200]))\n",
      "('Sample input: \\n', tensor([[    0,     0,     0,  ..., 34027, 34400, 42395],\n",
      "        [27396, 57185, 59205,  ..., 61788,  7060, 53166],\n",
      "        [65992,  6935, 71669,  ..., 21657, 29500, 40500],\n",
      "        ...,\n",
      "        [34296, 23859,  4088,  ...,   886, 68128,  2210],\n",
      "        [    0,     0,     0,  ..., 59205, 58499, 22288],\n",
      "        [    0,     0,     0,  ..., 66700, 19331, 28540]]))\n",
      "()\n",
      "('Sample label size: ', torch.Size([50]))\n",
      "('Sample label: \\n', tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0]))\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data and label. \n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available.\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model \n",
    "\n",
    "Here we are creating a simple RNN in PyTorch and pass the output to the a Linear layer and Sigmoid at the end to get the probability score and prediction as POSITIVE or NEGATIVE. \n",
    "\n",
    "The network is very similar to the CNN network created in Exercise 2. \n",
    "\n",
    "More info available at: https://pytorch.org/docs/0.3.1/nn.html?highlight=rnn#torch.nn.RNN\n",
    "\n",
    "Read about the parameters that the RNN takes and see what will happen when `batch_first` is set as `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(vocab_size, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # RNN out layer\n",
    "        print ('X-',x.shape, 'hidden',hidden.shape)\n",
    "        rnn_out, hidden = self.rnn(x, hidden)\n",
    "        print ('out-',rnn_out, 'hidden',hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        rnn_out = rnn_out.view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(rnn_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 7 : Know the shape\n",
    "\n",
    "Given a batch of 64 and input size as 1 and a sequence length of 200 to a RNN with 2 stacked layers and 512 hidden layers, find the shape of input data (x) and the hidden dimension (hidden) specified in the forward pass of the network. Note, the batch_first is kept to be True. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (rnn): RNN(74073, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
    "output_size = 1\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "#input shape = (64,200,vocab_size)\n",
    "#hidden shape = (2*1,64,256)\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 8: LSTM \n",
    "\n",
    "Before we start creating the LSTM, it is important to understand LSTM and to know why we prefer LSTM over a Vanilla RNN for this task. \n",
    "> Here are some good links to know about LSTM:\n",
    "* [Colah Blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "* [Understanding LSTM](http://blog.echen.me/2017/05/30/exploring-lstms/)\n",
    "* [RNN effectiveness](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "\n",
    "Now create a class named SentimentLSTM with `n_layers=2`, and rest all hyperparameters same as before. Also, create an embedding layer and feed the output of the embedding layer as input to the LSTM model. Dont forget to add a regularizer (dropout) layer after the LSTM layer with p=0.4 to prevent overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The LSTM model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # define embedding, LSTM, dropout and Linear layers here\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "       \n",
    "        self.lstm= nn.LSTM(embedding_dim,hidden_dim,num_layers=self.n_layers)\n",
    "       \n",
    "        self.fc = nn.Linear(in_features=hidden_dim, out_features=output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        # define embedding, LSTM, dropout and Linear layers here\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        # input = B x S . size(0) = B\n",
    "        batch_size = input.size(0)\n",
    "        seq_len = input.size(1)\n",
    "\n",
    "        # input:  B x S  -- (transpose) --> S x B\n",
    "        input = input.t()\n",
    "        \n",
    "        # Embedding Seq X Batch (50 x 200) ---->  Seq X Batch x E (50 x 200 x 300) (embedding size)\n",
    "        #print(\"  input\", input.size())\n",
    "        embedded = self.embedding(input)\n",
    "        #print(\"  embedding\", embedded.size())\n",
    "        \n",
    "       \n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "        #print('lstm output',lstm_out.size()) #('lstm hidden output', torch.Size([200, 50, 256]))\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        out = lstm_out.view(-1, self.hidden_dim)\n",
    "        #print('shaped output',out.size()) \n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(out)\n",
    "        fc_output = self.fc(out)\n",
    "        #print('fc_output',fc_output.size()) \n",
    "\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(fc_output)\n",
    "        #print('sig_out',sig_out.size(),sig_out) \n",
    "       \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        #print('reshaped sig_out',sig_out.size(),sig_out)\n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "      \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the network\n",
    "\n",
    "Here, we'll instantiate the network. First up, defining the hyperparameters.\n",
    "\n",
    "* `vocab_size`: Size of our vocabulary or the range of values for our input, word tokens.\n",
    "* `output_size`: Size of our desired output; the number of class scores we want to output (pos/neg).\n",
    "* `embedding_dim`: Number of columns in the embedding lookup table; size of our embeddings.\n",
    "* `hidden_dim`: Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
    "* `n_layers`: Number of LSTM layers in the network. Typically between 1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentLSTM(\n",
      "  (embedding): Embedding(74073, 300)\n",
      "  (lstm): LSTM(300, 256, num_layers=2)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with these hyperparameters\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
    "output_size = 1\n",
    "embedding_dim = 300\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Loss Functions\n",
    "We are using `BCELoss (Binary Cross Entropy Loss)` since we have two output classes. \n",
    "\n",
    "Can Cross Entropy Loss be used instead of BCELoss? \n",
    "\n",
    "If no, why not? If yes, how?\n",
    "\n",
    "Is `NLLLoss()` and last layer as `LogSoftmax()` is same as using `CrossEntropyLoss()` with a Softmax final layer? Can you get the mathematical intuition behind it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 1/4...', 'Step: 100...', 'Loss: 0.691256...', 'Val Loss: 0.692942')\n",
      "('Epoch: 1/4...', 'Step: 200...', 'Loss: 0.684352...', 'Val Loss: 0.694444')\n",
      "('Epoch: 1/4...', 'Step: 300...', 'Loss: 0.691322...', 'Val Loss: 0.693709')\n",
      "('Epoch: 1/4...', 'Step: 400...', 'Loss: 0.690494...', 'Val Loss: 0.693814')\n",
      "('Epoch: 2/4...', 'Step: 500...', 'Loss: 0.692708...', 'Val Loss: 0.694503')\n",
      "('Epoch: 2/4...', 'Step: 600...', 'Loss: 0.694232...', 'Val Loss: 0.694022')\n",
      "('Epoch: 2/4...', 'Step: 700...', 'Loss: 0.695717...', 'Val Loss: 0.693767')\n",
      "('Epoch: 2/4...', 'Step: 800...', 'Loss: 0.691160...', 'Val Loss: 0.693639')\n",
      "('Epoch: 3/4...', 'Step: 900...', 'Loss: 0.698660...', 'Val Loss: 0.693980')\n",
      "('Epoch: 3/4...', 'Step: 1000...', 'Loss: 0.699789...', 'Val Loss: 0.693398')\n",
      "('Epoch: 3/4...', 'Step: 1100...', 'Loss: 0.699838...', 'Val Loss: 0.694568')\n",
      "('Epoch: 3/4...', 'Step: 1200...', 'Loss: 0.694633...', 'Val Loss: 0.692897')\n",
      "('Epoch: 4/4...', 'Step: 1300...', 'Loss: 0.692366...', 'Val Loss: 0.693196')\n",
      "('Epoch: 4/4...', 'Step: 1400...', 'Loss: 0.691951...', 'Val Loss: 0.693881')\n",
      "('Epoch: 4/4...', 'Step: 1500...', 'Loss: 0.694256...', 'Val Loss: 0.692934')\n",
      "('Epoch: 4/4...', 'Step: 1600...', 'Loss: 0.699200...', 'Val Loss: 0.693753')\n"
     ]
    }
   ],
   "source": [
    "#Training and Validation\n",
    "\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "        #print (inputs.shape)\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        #break\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Once we are done with training and validating, we can improve training loss and validation loss by playing around with the hyperparameters. Can you find a better set of hyperparams? Play around with it. \n",
    "\n",
    "### Task 10: Prediction Function\n",
    "Now write a prediction function to predict the output for the test set created. Save the results in a CSV file with one column as the reviews and the prediction in the next column. Calculate the accuracy of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5028, 0.5044, 0.5190, 0.5139, 0.5145, 0.5106, 0.5304, 0.5155, 0.5016,\n",
      "        0.5067, 0.5245, 0.5079, 0.5089, 0.5094, 0.5155, 0.5145, 0.5014, 0.5047,\n",
      "        0.5152, 0.4993, 0.5212, 0.4933, 0.5165, 0.5407, 0.5122, 0.5141, 0.5274,\n",
      "        0.5021, 0.5181, 0.5097, 0.5109, 0.5120, 0.5068, 0.5136, 0.5147, 0.5160,\n",
      "        0.5149, 0.5259, 0.5042, 0.4945, 0.5350],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5151, 0.5007, 0.5117, 0.5189, 0.5186, 0.5030, 0.5257, 0.5162, 0.5203,\n",
      "        0.5163, 0.5156, 0.5143, 0.4883, 0.5007, 0.5112, 0.5063, 0.5249, 0.5198,\n",
      "        0.5442, 0.5204, 0.5130, 0.5217, 0.5190, 0.5169, 0.5140, 0.5118, 0.4942,\n",
      "        0.5242, 0.5247, 0.5287, 0.5073, 0.5222, 0.5046, 0.5275, 0.5114, 0.5287,\n",
      "        0.5200, 0.5207, 0.5146, 0.5128, 0.5052, 0.5100, 0.4934, 0.5064, 0.5078,\n",
      "        0.4988, 0.4946, 0.5110, 0.5186, 0.5173],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.4908,\n",
      "        0.5078, 0.4983, 0.5067, 0.5020, 0.4984, 0.5271, 0.5221, 0.5169, 0.5066,\n",
      "        0.5029, 0.4933, 0.5129, 0.5145, 0.5167],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.4976, 0.5013, 0.5157, 0.5317, 0.5177, 0.4827, 0.5042, 0.5222, 0.5209,\n",
      "        0.5183, 0.5153, 0.5026, 0.5165, 0.5143, 0.5061, 0.5123, 0.5377, 0.5108,\n",
      "        0.5102, 0.5088, 0.5397, 0.5106, 0.5076, 0.4863, 0.4992, 0.4923, 0.4978,\n",
      "        0.4888, 0.5116, 0.5046, 0.5034, 0.5068],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.4951, 0.5069, 0.5319, 0.5151, 0.5150, 0.5096, 0.4986,\n",
      "        0.5139, 0.5133, 0.5145, 0.5159, 0.5168, 0.5133, 0.5237, 0.5125, 0.5240,\n",
      "        0.5188, 0.5080, 0.5085, 0.5145, 0.5075, 0.5138, 0.5073, 0.4923, 0.5124,\n",
      "        0.5092, 0.5052, 0.5078, 0.5083, 0.4950],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.4907, 0.5022, 0.5082, 0.4879,\n",
      "        0.4967, 0.5241, 0.5050, 0.5165, 0.5203, 0.5014, 0.5014, 0.5096, 0.5037,\n",
      "        0.5110, 0.5136, 0.5021, 0.5230, 0.5206, 0.4997, 0.5047, 0.5202, 0.5009,\n",
      "        0.4893, 0.5067, 0.5196, 0.5207, 0.5099],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.4982,\n",
      "        0.5092, 0.5340, 0.5137, 0.5243, 0.4996, 0.5361, 0.5187, 0.4987, 0.5191,\n",
      "        0.5199, 0.5078, 0.5170, 0.5128, 0.5072, 0.5023, 0.4966, 0.5237, 0.5175,\n",
      "        0.5110, 0.5076, 0.5126, 0.5189, 0.5159, 0.5125, 0.4832, 0.5084, 0.5176,\n",
      "        0.5224, 0.5124, 0.5159, 0.5039, 0.4980],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5035, 0.5226, 0.5068, 0.5164, 0.5135, 0.5117, 0.5123, 0.5175, 0.5163,\n",
      "        0.5002, 0.5085, 0.5113, 0.5281, 0.5273, 0.5102, 0.5056, 0.5190, 0.4999,\n",
      "        0.5182, 0.5131, 0.5080, 0.4998, 0.5180, 0.5357, 0.5353, 0.5091, 0.5237,\n",
      "        0.5282, 0.5207, 0.5059, 0.5371, 0.5300, 0.5204, 0.5014, 0.5198, 0.5205,\n",
      "        0.5015, 0.5187, 0.5037, 0.5247, 0.5212, 0.5163, 0.5201, 0.5254, 0.5288,\n",
      "        0.5024, 0.5128, 0.5004, 0.5086, 0.5285],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.4920, 0.5221, 0.5294, 0.4987, 0.5012, 0.5177, 0.5040, 0.5095,\n",
      "        0.5003, 0.5212, 0.5193, 0.5188, 0.5146, 0.5051, 0.4952, 0.5023, 0.5189,\n",
      "        0.5163, 0.5485, 0.5146, 0.4995, 0.4914, 0.5002, 0.4927, 0.5054, 0.5136,\n",
      "        0.5117, 0.4901, 0.5205, 0.5261, 0.5154, 0.5384, 0.5173, 0.5129, 0.5191,\n",
      "        0.5158, 0.5089, 0.5144, 0.5109, 0.5280, 0.5207, 0.5075, 0.5198, 0.5048,\n",
      "        0.5226, 0.5166, 0.4983, 0.4873, 0.5052],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5015, 0.5015, 0.5015, 0.5015, 0.5183,\n",
      "        0.4902, 0.5036, 0.5010, 0.4998, 0.5067, 0.5052, 0.5130, 0.5246, 0.5097,\n",
      "        0.4954, 0.5101, 0.5091, 0.5086, 0.5118, 0.5102, 0.5144, 0.5036, 0.5128,\n",
      "        0.5095, 0.5081, 0.5073, 0.5082, 0.5212, 0.5151, 0.5149, 0.5178, 0.5086,\n",
      "        0.5110, 0.5057, 0.4984, 0.5112, 0.5105, 0.5191, 0.5343, 0.4930, 0.5017,\n",
      "        0.5079, 0.5186, 0.4929, 0.5314, 0.5348],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5311, 0.5099, 0.5079, 0.5077, 0.5231, 0.5253, 0.5161, 0.5063, 0.5250,\n",
      "        0.5103, 0.4917, 0.4845, 0.4958, 0.4996, 0.5090, 0.4861, 0.5076, 0.5063,\n",
      "        0.5083, 0.5135, 0.5217, 0.5054, 0.4980, 0.5212, 0.5240, 0.5080, 0.5143,\n",
      "        0.5151, 0.5199, 0.5324, 0.5138, 0.5169, 0.5264, 0.5225, 0.5007, 0.5125,\n",
      "        0.5318, 0.4857, 0.5190, 0.5156, 0.5246, 0.5161, 0.5027, 0.5085, 0.5102,\n",
      "        0.5176, 0.4997, 0.5169, 0.5098, 0.5091],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5239, 0.5000, 0.5299, 0.5095, 0.5105, 0.5063, 0.4983, 0.5215, 0.5111,\n",
      "        0.5169, 0.5156, 0.5209, 0.5081, 0.5276, 0.5004, 0.5359, 0.5138, 0.5018,\n",
      "        0.5041, 0.5023, 0.5094, 0.5356, 0.5238, 0.5180, 0.5095, 0.5276, 0.4909,\n",
      "        0.5166, 0.5164, 0.5075, 0.4976, 0.5072, 0.5075, 0.5037, 0.5207, 0.5052,\n",
      "        0.5058, 0.5124, 0.5202, 0.5226, 0.5107, 0.5456, 0.5102, 0.4958, 0.5116,\n",
      "        0.4997, 0.5163, 0.4987, 0.4968, 0.5075],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5072, 0.5013, 0.5010, 0.5012, 0.5154,\n",
      "        0.5094, 0.5133, 0.5237, 0.4970, 0.4994, 0.5164, 0.5073, 0.5197, 0.5026,\n",
      "        0.4979, 0.5080, 0.5051, 0.5047, 0.5069, 0.5035, 0.5183, 0.5084, 0.4969,\n",
      "        0.5158, 0.4826, 0.5140, 0.5059, 0.5066, 0.5213, 0.5105, 0.5097, 0.5055,\n",
      "        0.4902, 0.5101, 0.4919, 0.5021, 0.5105],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5000, 0.4897, 0.4982, 0.5099, 0.5046, 0.5012, 0.5186, 0.5168,\n",
      "        0.5149, 0.5189, 0.4842, 0.5225, 0.5125, 0.5121, 0.5304, 0.5161, 0.4961,\n",
      "        0.5064, 0.5103, 0.4970, 0.5046, 0.4992, 0.5034, 0.5049, 0.5135, 0.5323,\n",
      "        0.5084, 0.5132, 0.5103, 0.5352, 0.5136, 0.5339, 0.5177, 0.5166, 0.5016,\n",
      "        0.5208, 0.5163, 0.5258, 0.4954, 0.4934],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.4952, 0.5032, 0.5158, 0.5092, 0.5158, 0.5220, 0.5155, 0.5111, 0.5054,\n",
      "        0.5129, 0.5077, 0.5078, 0.5176, 0.5110, 0.5041, 0.5053, 0.4983, 0.4966,\n",
      "        0.5036, 0.5098, 0.5129, 0.5195, 0.5151, 0.4927, 0.5091, 0.5018, 0.5142,\n",
      "        0.5161, 0.5132, 0.5075, 0.4942, 0.5210, 0.5009, 0.5145, 0.5081, 0.5280,\n",
      "        0.5252, 0.5175, 0.5120, 0.5056, 0.5341, 0.5141, 0.5020, 0.5112, 0.5157,\n",
      "        0.5038, 0.5087, 0.5048, 0.5244, 0.5011],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5075, 0.5202, 0.5104, 0.5061, 0.5122, 0.5241, 0.5142, 0.5038, 0.5258,\n",
      "        0.4984, 0.5206, 0.5106, 0.5085, 0.5098, 0.5509, 0.5260, 0.5229, 0.5212,\n",
      "        0.5043, 0.5084, 0.5178, 0.5074, 0.4950, 0.5001, 0.5038, 0.5064, 0.5059,\n",
      "        0.5072, 0.5245, 0.5017, 0.5084, 0.5093, 0.5054, 0.5170, 0.5076, 0.5220,\n",
      "        0.5047, 0.5125, 0.5212, 0.5116, 0.5080, 0.5020, 0.5122, 0.5052, 0.5036,\n",
      "        0.5199, 0.5162, 0.4990, 0.5002, 0.5000],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.4970, 0.5225, 0.5036, 0.5212, 0.5125, 0.5159, 0.5201, 0.5216, 0.5032,\n",
      "        0.5088, 0.5090, 0.5231, 0.5230, 0.5114, 0.5138, 0.4947, 0.5157, 0.5144,\n",
      "        0.5052, 0.5362, 0.5230, 0.4953, 0.5179, 0.5056, 0.5258, 0.5254, 0.5086,\n",
      "        0.5203, 0.5296, 0.5214, 0.5234, 0.5448, 0.5106, 0.5334, 0.5233, 0.5205,\n",
      "        0.5272, 0.5335, 0.5289, 0.5206, 0.4996, 0.5147, 0.5134, 0.5090, 0.5093,\n",
      "        0.5201, 0.5192, 0.5335, 0.5243, 0.5117],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5193, 0.5114, 0.5036, 0.4871, 0.5138, 0.4934, 0.5097, 0.5208, 0.5119,\n",
      "        0.5134, 0.5198, 0.5100, 0.4982, 0.5034, 0.5021, 0.5228, 0.5054, 0.5071,\n",
      "        0.4886, 0.5114, 0.5142, 0.5181, 0.5121, 0.4993, 0.5167, 0.4984, 0.5071,\n",
      "        0.5070, 0.5071, 0.4963, 0.5134, 0.5067, 0.5120, 0.4987, 0.5097, 0.5116,\n",
      "        0.5153, 0.5171, 0.5131, 0.5178, 0.5114, 0.4911, 0.5045, 0.5191, 0.5169,\n",
      "        0.5106, 0.5125, 0.5150, 0.5039, 0.4987],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5051, 0.4992, 0.4946,\n",
      "        0.5404, 0.5130, 0.5234, 0.5161, 0.5090, 0.4995, 0.5061, 0.5118, 0.5143,\n",
      "        0.5207, 0.5041, 0.5147, 0.5182, 0.4921, 0.4873, 0.5137, 0.5099, 0.5080,\n",
      "        0.4928, 0.4947, 0.5172, 0.5110, 0.4975, 0.5131, 0.5227, 0.5093, 0.4981,\n",
      "        0.5056, 0.5109, 0.5091, 0.5140, 0.4886],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5090, 0.5131, 0.5098, 0.4993, 0.5128, 0.5058, 0.5091, 0.5082, 0.5154,\n",
      "        0.5099, 0.5352, 0.5065, 0.5003, 0.5067, 0.4985, 0.5149, 0.5188, 0.5018,\n",
      "        0.5093, 0.4955, 0.5111, 0.5150, 0.4869, 0.5012, 0.5029, 0.5093, 0.5117,\n",
      "        0.5360, 0.5167, 0.4970, 0.5059, 0.5283, 0.5007, 0.5136, 0.5047, 0.5039,\n",
      "        0.4886, 0.5098, 0.5186, 0.5354, 0.5145, 0.4985, 0.5064, 0.5131, 0.4955,\n",
      "        0.4989, 0.5024, 0.5176, 0.5184, 0.5144],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5143, 0.5010, 0.5169, 0.4872, 0.4986, 0.4893, 0.5130, 0.5129, 0.5150,\n",
      "        0.4950, 0.5110, 0.5154, 0.5035, 0.5232, 0.5218, 0.5036, 0.4973, 0.4927,\n",
      "        0.5161, 0.5095, 0.5099, 0.4966, 0.5179, 0.5222, 0.5095, 0.5028, 0.4942,\n",
      "        0.4988, 0.5064, 0.5146, 0.5230, 0.5079, 0.5128, 0.5055, 0.4954, 0.5098,\n",
      "        0.5125, 0.5050, 0.5186, 0.5115, 0.5074, 0.5112, 0.4963, 0.5117, 0.5105,\n",
      "        0.4991, 0.5075, 0.5126, 0.5017, 0.5378],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5198, 0.5143, 0.4926, 0.5044, 0.5017, 0.5192, 0.5149, 0.5239, 0.5094,\n",
      "        0.5075, 0.5139, 0.4978, 0.5472, 0.5059, 0.4980, 0.5144, 0.5139, 0.5167,\n",
      "        0.5150, 0.5077, 0.5184, 0.5143, 0.5300, 0.5156, 0.5083, 0.4931, 0.5060,\n",
      "        0.5014, 0.5094, 0.5048, 0.5076, 0.5154, 0.5190, 0.5212, 0.5133, 0.5142,\n",
      "        0.5113, 0.5078, 0.5495, 0.5207, 0.5245, 0.4975, 0.4960, 0.5100, 0.4960,\n",
      "        0.5127, 0.5210, 0.5118, 0.5068, 0.5046],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5110, 0.5094, 0.5111, 0.4952, 0.5242, 0.4943, 0.5100, 0.5098, 0.5155,\n",
      "        0.5121, 0.5244, 0.4950, 0.5063, 0.5137, 0.5112, 0.5400, 0.5085, 0.5259,\n",
      "        0.5091, 0.5040, 0.4977, 0.5028, 0.5476, 0.5001, 0.5000, 0.5160, 0.5125,\n",
      "        0.5089, 0.5072, 0.5144, 0.5166, 0.5020, 0.5058, 0.4983, 0.5201, 0.5148,\n",
      "        0.5079, 0.4938, 0.4731, 0.4930, 0.5258, 0.5004, 0.4989, 0.5314, 0.5071,\n",
      "        0.4993, 0.5043, 0.5232, 0.5053, 0.4949],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5175, 0.5161, 0.5065, 0.5142, 0.5066, 0.5120, 0.5076, 0.5280, 0.4948,\n",
      "        0.4859, 0.5241, 0.5144, 0.5384, 0.5012, 0.5013, 0.5023, 0.5090, 0.5334,\n",
      "        0.5004, 0.5106, 0.5121, 0.4990, 0.4906, 0.5230, 0.5320, 0.5105, 0.5199,\n",
      "        0.5109, 0.5086, 0.5045, 0.5173, 0.5194, 0.5292, 0.4989, 0.5050, 0.5219,\n",
      "        0.5136, 0.5024, 0.5091, 0.5009, 0.4955, 0.5081, 0.5108, 0.5023, 0.5064,\n",
      "        0.5071, 0.5171, 0.5075, 0.5106, 0.5066],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5136, 0.4877, 0.4855, 0.5196, 0.5327, 0.5132, 0.4845, 0.5109, 0.5016,\n",
      "        0.5092, 0.4967, 0.5035, 0.5038, 0.5000, 0.5136, 0.5090, 0.5096, 0.5148,\n",
      "        0.5247, 0.5148, 0.5207, 0.5256, 0.5118, 0.5017, 0.5021, 0.5097, 0.5090,\n",
      "        0.5003, 0.5309, 0.5010, 0.5065, 0.5062, 0.5125, 0.5067, 0.5079, 0.4958,\n",
      "        0.5038, 0.5072, 0.5135, 0.5172, 0.5177],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5003, 0.4829, 0.5185, 0.5214, 0.5051, 0.4964, 0.5111, 0.5122, 0.5032,\n",
      "        0.5153, 0.5142, 0.4997, 0.5075, 0.4943, 0.4937, 0.5172, 0.5327, 0.5155,\n",
      "        0.5019, 0.5125, 0.5033, 0.5089, 0.5310, 0.5557, 0.5146, 0.5278, 0.5110,\n",
      "        0.5065, 0.5151, 0.5212, 0.4998, 0.5222, 0.5113, 0.5203, 0.5174, 0.5092,\n",
      "        0.5020, 0.4974, 0.4960, 0.5088, 0.5204, 0.5357, 0.5233, 0.5139, 0.5160,\n",
      "        0.5036, 0.5167, 0.5008, 0.5220, 0.5233],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.4981, 0.5017, 0.5061, 0.5074, 0.5151, 0.5102, 0.5061, 0.4801,\n",
      "        0.4931, 0.5117, 0.5030, 0.5162, 0.5068, 0.5097, 0.5151, 0.4932, 0.5095,\n",
      "        0.5186, 0.5148, 0.5028, 0.4985, 0.4843, 0.4962, 0.5083, 0.5182, 0.5038,\n",
      "        0.5013, 0.5326, 0.5107, 0.5290, 0.5108],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5382, 0.4957, 0.4990, 0.5249, 0.4970, 0.5381, 0.5008, 0.5004, 0.5072,\n",
      "        0.5082, 0.5131, 0.5225, 0.5178, 0.5178, 0.5172, 0.5153, 0.5141, 0.4814,\n",
      "        0.5287, 0.5234, 0.5110, 0.5063, 0.5117, 0.4998, 0.5068, 0.5313, 0.5127,\n",
      "        0.5163, 0.5074, 0.5017, 0.5117, 0.4991, 0.5149, 0.5112, 0.4907, 0.5033,\n",
      "        0.4983, 0.5044, 0.4983, 0.5118, 0.5074, 0.4878, 0.5140, 0.5081, 0.5002,\n",
      "        0.4997, 0.5240, 0.5286, 0.5061, 0.5073],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5083, 0.5195, 0.5113,\n",
      "        0.5355, 0.5137, 0.5239, 0.5018, 0.5189, 0.5142, 0.5263, 0.5151, 0.5234,\n",
      "        0.5083, 0.5144, 0.5075, 0.5092, 0.5111, 0.5081, 0.5293, 0.5179, 0.5122,\n",
      "        0.5008, 0.5124, 0.5076, 0.5149, 0.5004, 0.5268, 0.5169, 0.5164, 0.5025,\n",
      "        0.5169, 0.5183, 0.5153, 0.5047, 0.5177],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5135, 0.5103, 0.5190, 0.5028, 0.4988, 0.5012, 0.5140, 0.5293,\n",
      "        0.5133, 0.5055, 0.5183, 0.5203, 0.5216],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5051, 0.5006, 0.5139, 0.5197, 0.5032, 0.5049, 0.5096, 0.5109, 0.5172,\n",
      "        0.5146, 0.5076, 0.5177, 0.5206, 0.5087, 0.5220, 0.5047, 0.5030, 0.4922,\n",
      "        0.5139, 0.5230, 0.5231, 0.4907, 0.5095, 0.5044, 0.5178, 0.5121, 0.5027,\n",
      "        0.5103, 0.4997, 0.4954, 0.4998, 0.4956, 0.5033, 0.5079, 0.5131, 0.4943,\n",
      "        0.5134, 0.5298, 0.5004, 0.5118, 0.5217, 0.5026, 0.4833, 0.4911, 0.5083,\n",
      "        0.5057, 0.5171, 0.5207, 0.5051, 0.4974],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5015, 0.5015, 0.5015, 0.5015, 0.4928,\n",
      "        0.5279, 0.5035, 0.5147, 0.5185, 0.5033, 0.4907, 0.4996, 0.4992, 0.5228,\n",
      "        0.5103, 0.5031, 0.5146, 0.5042, 0.5141, 0.5065, 0.5040, 0.5073, 0.5035,\n",
      "        0.5148, 0.5041, 0.5101, 0.5118, 0.5273, 0.5259, 0.5152, 0.4986, 0.5094,\n",
      "        0.5180, 0.5133, 0.5198, 0.5098, 0.5265, 0.5047, 0.5085, 0.5048, 0.5046,\n",
      "        0.5175, 0.5112, 0.4969, 0.5037, 0.5108],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5105, 0.4902, 0.5114, 0.5220, 0.5128, 0.5339,\n",
      "        0.5090, 0.5236, 0.5104, 0.5184, 0.5130, 0.5152, 0.5137, 0.5128, 0.5152,\n",
      "        0.5138, 0.5176, 0.5101, 0.5031, 0.5121, 0.5027, 0.5123, 0.5215, 0.5149,\n",
      "        0.5085, 0.5090, 0.5166, 0.4949, 0.5038],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5053, 0.5084, 0.5234, 0.5341, 0.5040, 0.5109, 0.5056, 0.5113, 0.5062,\n",
      "        0.5165, 0.5194, 0.5235, 0.5329, 0.5167, 0.5102, 0.5478, 0.5095, 0.5056,\n",
      "        0.5172, 0.5179, 0.5160, 0.5177, 0.5082, 0.5185, 0.5147, 0.5076, 0.5120,\n",
      "        0.5104, 0.5084, 0.5414, 0.5194, 0.5162, 0.5179, 0.5026, 0.5123, 0.5051,\n",
      "        0.5090, 0.5262, 0.5147, 0.5069, 0.5097, 0.4984, 0.5125, 0.5260, 0.5171,\n",
      "        0.5488, 0.5065, 0.5106, 0.5104, 0.5127],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5181, 0.5085,\n",
      "        0.4909, 0.4744, 0.5080, 0.4977, 0.4981, 0.5127, 0.5187, 0.5035, 0.5160,\n",
      "        0.4970, 0.5007, 0.5213, 0.5036, 0.4998],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5047, 0.5147, 0.5048, 0.5086, 0.4975,\n",
      "        0.4912, 0.5142, 0.5055, 0.5161, 0.5304, 0.5141, 0.5062, 0.5059, 0.5150,\n",
      "        0.5207, 0.5338, 0.5160, 0.5314, 0.5113, 0.5205, 0.5178, 0.5076, 0.5106,\n",
      "        0.5207, 0.5073, 0.4938, 0.5120, 0.5156, 0.5197, 0.5240, 0.5350, 0.5054,\n",
      "        0.5033, 0.4991, 0.5098, 0.5157, 0.4973, 0.5036, 0.5005, 0.5055, 0.5074,\n",
      "        0.4926, 0.5082, 0.5033, 0.4996, 0.4804],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5137, 0.5056, 0.4804, 0.5067, 0.5124, 0.5109, 0.5050, 0.5006, 0.4876,\n",
      "        0.4808, 0.5060, 0.4857, 0.5253, 0.5096, 0.5017, 0.5093, 0.5095, 0.4912,\n",
      "        0.4963, 0.5039, 0.5010, 0.4976, 0.5067, 0.5077, 0.4938, 0.5067, 0.4934,\n",
      "        0.5065, 0.5088, 0.5172, 0.5210, 0.5259, 0.5075, 0.5083, 0.4953, 0.5021,\n",
      "        0.5024, 0.5166, 0.5143, 0.5059, 0.5096, 0.5140, 0.5098, 0.5163, 0.5105,\n",
      "        0.5178, 0.5169, 0.5032, 0.4998, 0.5051],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.5164, 0.5314, 0.5366, 0.5087, 0.4944, 0.5710, 0.5428, 0.4913,\n",
      "        0.5153, 0.5139, 0.5071, 0.5162, 0.5407, 0.5124, 0.5036, 0.4991, 0.5127,\n",
      "        0.5065, 0.5449, 0.5152, 0.5399, 0.5344, 0.5179, 0.5233, 0.5214, 0.5170,\n",
      "        0.5067, 0.5048, 0.5279, 0.5029, 0.5168, 0.5263, 0.5121, 0.5137, 0.5278,\n",
      "        0.5192, 0.5242, 0.4990, 0.5051, 0.5049, 0.4923, 0.5116, 0.5188, 0.5218,\n",
      "        0.5190, 0.4946, 0.5157, 0.5184, 0.5227],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5171, 0.5033, 0.5143, 0.5231, 0.5163, 0.5063, 0.5133, 0.5187, 0.5040,\n",
      "        0.5101, 0.5289, 0.5197, 0.5082, 0.5060, 0.5081, 0.5032, 0.5066, 0.5036,\n",
      "        0.4992, 0.5058, 0.5151, 0.5119, 0.5495, 0.5216, 0.5179, 0.5306, 0.5186,\n",
      "        0.5035, 0.5373, 0.5192, 0.5098, 0.5190, 0.5031, 0.5068, 0.5246, 0.5024,\n",
      "        0.5026, 0.5030, 0.5009, 0.4909, 0.5125, 0.4923, 0.5152, 0.5074, 0.5097,\n",
      "        0.5152, 0.5095, 0.5046, 0.5152, 0.5039],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5145, 0.5062, 0.5041, 0.5252, 0.4975, 0.5147,\n",
      "        0.5082, 0.5109, 0.5125, 0.4997, 0.4718],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.4977,\n",
      "        0.5124, 0.5025, 0.5051, 0.5103, 0.4929, 0.5128, 0.5058, 0.5032, 0.5240,\n",
      "        0.4985, 0.5172, 0.5049, 0.4996, 0.5293, 0.5195, 0.5058, 0.5122, 0.5031,\n",
      "        0.4960, 0.5086, 0.5178, 0.5136, 0.5173, 0.5010, 0.4983, 0.5148, 0.5041,\n",
      "        0.4965, 0.5055, 0.5217, 0.5128, 0.5337],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.4994, 0.5166, 0.5031, 0.5066, 0.5083, 0.4976, 0.5141, 0.5066, 0.5003,\n",
      "        0.5134, 0.5170, 0.5011, 0.5191, 0.5205, 0.5101, 0.5227, 0.5188, 0.5387,\n",
      "        0.5038, 0.4801, 0.5062, 0.5320, 0.4926, 0.5090, 0.5085, 0.5055, 0.5165,\n",
      "        0.5048, 0.5028, 0.4929, 0.5095, 0.4970, 0.5074, 0.5163, 0.5138, 0.5036,\n",
      "        0.5161, 0.5173, 0.5115, 0.4991, 0.5120, 0.5272, 0.4759, 0.5157, 0.5032,\n",
      "        0.5126, 0.5230, 0.5027, 0.5113, 0.5198],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5094, 0.4912, 0.5030, 0.4993, 0.4787, 0.5210, 0.5008, 0.5081, 0.5105,\n",
      "        0.5090, 0.4972, 0.5170, 0.5140, 0.5306, 0.5295, 0.5177, 0.5333, 0.5202,\n",
      "        0.5210, 0.5261, 0.5120, 0.5150, 0.5106, 0.5161, 0.5054, 0.5132, 0.4950,\n",
      "        0.4997, 0.5235, 0.4997, 0.5172, 0.5137, 0.5072, 0.5208, 0.5235, 0.5126,\n",
      "        0.5366, 0.5121, 0.5179, 0.5166, 0.5012, 0.4945, 0.4940, 0.4948, 0.5125,\n",
      "        0.5001, 0.5058, 0.4867, 0.5009, 0.5465],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5133, 0.5318, 0.5072, 0.5083, 0.5223, 0.5154, 0.5054, 0.5134, 0.5135,\n",
      "        0.5076, 0.5082, 0.5069, 0.5225, 0.5073, 0.5186, 0.5150, 0.5056, 0.5217,\n",
      "        0.5068, 0.5275, 0.5241, 0.5174, 0.5187, 0.5110, 0.5178, 0.5211, 0.5203,\n",
      "        0.4979, 0.5133, 0.5103, 0.5087, 0.5244, 0.5240, 0.5168, 0.5082, 0.5230,\n",
      "        0.5241, 0.5087, 0.5199, 0.5092, 0.5163, 0.4987, 0.5124, 0.4974, 0.5046,\n",
      "        0.5209, 0.5107, 0.5151, 0.5145, 0.5368],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5015, 0.5015, 0.5015, 0.4963, 0.5081,\n",
      "        0.5063, 0.5106, 0.5100, 0.5143, 0.5108, 0.5144, 0.5190, 0.5292, 0.4961,\n",
      "        0.5008, 0.5178, 0.5092, 0.5220, 0.5146, 0.5148, 0.5342, 0.5239, 0.5262,\n",
      "        0.5349, 0.5316, 0.5064, 0.5205, 0.5229, 0.5208, 0.5062, 0.5044, 0.5186,\n",
      "        0.5087, 0.5029, 0.5060, 0.5311, 0.4982, 0.5111, 0.5120, 0.5223, 0.5203,\n",
      "        0.5166, 0.5078, 0.5070, 0.5156, 0.5081],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5102, 0.5086, 0.5229, 0.5154, 0.5388, 0.5092, 0.5107, 0.5239, 0.5100,\n",
      "        0.5349, 0.5261, 0.5090, 0.5109, 0.5063, 0.5251, 0.5092, 0.4982, 0.5111,\n",
      "        0.5152, 0.5140, 0.5029, 0.5155, 0.5393, 0.5056, 0.5302, 0.5038, 0.5159,\n",
      "        0.5055, 0.5175, 0.4931, 0.5175, 0.5136, 0.5156, 0.5146, 0.5180, 0.5052,\n",
      "        0.5289, 0.5148, 0.5372, 0.5058, 0.5066, 0.5311, 0.5074, 0.5324, 0.5012,\n",
      "        0.5199, 0.5127, 0.5127, 0.4970, 0.5037],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5315, 0.5090, 0.5203, 0.5034, 0.5103, 0.4984, 0.5218, 0.5260, 0.4985,\n",
      "        0.5122, 0.4990, 0.5220, 0.5017, 0.5110, 0.5371, 0.5174, 0.5068, 0.5025,\n",
      "        0.5168, 0.5365, 0.5149, 0.5152, 0.4953, 0.5057, 0.5172, 0.5137, 0.5181,\n",
      "        0.5118, 0.5051, 0.5105, 0.5184, 0.4985, 0.5341, 0.5000, 0.5041, 0.5089,\n",
      "        0.5169, 0.5077, 0.5162, 0.5077, 0.5117, 0.5205, 0.5042, 0.5044, 0.5101,\n",
      "        0.5084, 0.5186, 0.5224, 0.5186, 0.5272],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5131, 0.5207, 0.5169, 0.4943, 0.5076, 0.5210, 0.5291, 0.4958, 0.5219,\n",
      "        0.5076, 0.5109, 0.5163, 0.5460, 0.5168, 0.4963, 0.5015, 0.5187, 0.5167,\n",
      "        0.5163, 0.5217, 0.5084, 0.5137, 0.5131, 0.5122, 0.5049, 0.5303, 0.5120,\n",
      "        0.5292, 0.5119, 0.5168, 0.5192, 0.5042, 0.4946, 0.4985, 0.5016, 0.5026,\n",
      "        0.5120, 0.5005, 0.5364, 0.5429, 0.4969, 0.5281, 0.4923, 0.5055, 0.5093,\n",
      "        0.5286, 0.5245, 0.4956, 0.5221, 0.5376],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5245, 0.5061, 0.5068, 0.5080, 0.5193, 0.5152, 0.5091, 0.5196, 0.5098,\n",
      "        0.5157, 0.5173, 0.5255, 0.5070, 0.5199, 0.5094, 0.5094, 0.5313, 0.5166,\n",
      "        0.5210, 0.5096, 0.5252, 0.5152, 0.5003, 0.4953, 0.4992, 0.5012, 0.5030,\n",
      "        0.5155, 0.5073, 0.5047, 0.5267, 0.5141, 0.5136, 0.5153, 0.5060, 0.5348,\n",
      "        0.5224, 0.5116, 0.5421, 0.5008, 0.5007, 0.5106, 0.5120, 0.5067, 0.5007,\n",
      "        0.5068, 0.5185, 0.5039, 0.5221, 0.5188],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.5031, 0.5016, 0.5016, 0.5016, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015, 0.5015,\n",
      "        0.5015, 0.4997, 0.5116, 0.5243, 0.5096, 0.4996, 0.5100, 0.5005, 0.5188,\n",
      "        0.5141, 0.4996, 0.5008, 0.5066, 0.5143, 0.5121, 0.5097, 0.5089, 0.5019,\n",
      "        0.5187, 0.5136, 0.5081, 0.5346, 0.5139, 0.5175, 0.4966, 0.4829, 0.5251,\n",
      "        0.5151, 0.5246, 0.5251, 0.5087, 0.5288],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "def predict():\n",
    "    net.eval()\n",
    "    label_dict = {}\n",
    "    label_dict[0] = 'positive'\n",
    "    label_dict[1] = 'negative'\n",
    "    test_h = net.init_hidden(batch_size)\n",
    "    prediction = []\n",
    "    for inputs,labels in test_loader:\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        test_h = tuple([each.data for each in val_h])\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        output, test_h = net(inputs, val_h)\n",
    "        print(output)\n",
    "        #prediction.append(label_dict[])\n",
    "        \n",
    "\n",
    "                \n",
    "\n",
    "predict()                \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question: Create an app using Flask\n",
    "\n",
    "> Extra bonus points if someone attempts this question:\n",
    "* Save the trained model checkpoints.\n",
    "* Create a Flask app and load the model. A similar work in the field of CNN has been done here : https://github.com/kumar-shridhar/Business-Card-Detector (Check `app.py`)\n",
    "* You can use hosting services like Heroku and/or with Docker to host your app and show it to everyone. \n",
    "Example here: https://github.com/selimrbd/sentiment_analysis/blob/master/Dockerfile\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
